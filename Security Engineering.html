<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "XHTML1-s.dtd" >
<html xmlns="http://www.w3.org/TR/1999/REC-html-in-xml" xml:lang="en" lang="en">

<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>
<style>
.bodyContainer {
    font-family: Arial, Helvetica, sans-serif;
    text-align: center;
    padding-left: 32px;
    padding-right: 32px;
}

.notebookFor {
    font-size: 18px;
    font-weight: 700;
    text-align: center;
    color: rgb(119, 119, 119);
    margin: 24px 0px 0px;
    padding: 0px;
}

.bookTitle {
    font-size: 32px;
    font-weight: 700;
    text-align: center;
    color: #333333;
    margin-top: 22px;
    padding: 0px;
}

.authors {
    font-size: 13px;
    font-weight: 700;
    text-align: center;
    color: rgb(119, 119, 119);
    margin-top: 22px;
    margin-bottom: 24px; 
    padding: 0px;
}

.sectionHeading {
    font-size: 24px;
    font-weight: 700;
    text-align: left;
    color: #333333;
    margin-top: 24px;
    padding: 0px;
}

.noteHeading {
    font-size: 18px;
    font-weight: 700;
    text-align: left;
    color: #333333;
    margin-top: 20px;
    padding: 0px;
}

.noteText {
    font-size: 18px;
    font-weight: 500;
    text-align: left;
    color: #333333;
    margin: 2px 0px 0px;
    padding: 0px;
}

.highlight_blue {
    color: rgb(178, 205, 251);
}

.highlight_orange {
    color: #ffd7ae;
}

.highlight_pink {
    color: rgb(255, 191, 206);
}

.highlight_yellow {
    color: rgb(247, 206, 0);
}

.notebookGraphic {
    margin-top: 10px;
    text-align: left;
}

.notebookGraphic img {
    -o-box-shadow:      0px 0px 5px #888;
    -icab-box-shadow:   0px 0px 5px #888;
    -khtml-box-shadow:  0px 0px 5px #888;
    -moz-box-shadow:    0px 0px 5px #888;
    -webkit-box-shadow: 0px 0px 5px #888;
    box-shadow:         0px 0px 5px #888; 
    max-width: 100%;
    height: auto;
}

hr {
    border: 0px none;
    height: 1px;
    background: none repeat scroll 0% 0% rgb(221, 221, 221);
}
</style>
</head>
<body>
<div class='bodyContainer'>
<h1><div class='notebookFor'>Notes and highlights for</div><div class='bookTitle'>Security Engineering _ A Guide to Building Dependable Distributed Systems (9781119642817) by Anderson Ross
</div><div class='authors'>
Anderson, Ross
</div></h1><hr/>

<h2 class='sectionHeading'>PART I</h2><h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 2: Who Is the Opponent? &gt; Location 2460</div><div class='noteText'>In 2019, this credential stuffing still accounts for the largest number of attempted account compromises by volume</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 2: Who Is the Opponent? &gt; Location 2466</div><div class='noteText'>A variant on the theme is the pay-per-install service, which implants malware on phones or PCs to order and at scale. This can involve a range of phishing lures in a variety of contexts, from free porn sites that ask you to install a special viewer, to sports paraphernalia offers and news about topical events. It can also use more technical means such as drive-by downloads. Such services are often offered by botnets which need them to maintain their own numbers; they might charge third party customers $ 10-15 per thousand machines infected in the USA and Europe, and perhaps $ 3 for Asia.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 2: Who Is the Opponent? &gt; Location 2472</div><div class='noteText'>hack-for-hire operators who will try to compromise a specific target account for a fee, of typically $ 750</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 3: Psychology and Usability &gt; Location 3750</div><div class='noteText'>For users who follow instructions, passwords based on mnemonic phrases offer the best of both worlds. They are as easy to remember as naively selected passwords, and as hard to guess as random passwords.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 3: Psychology and Usability &gt; Location 3751</div><div class='noteText'>The problem then becomes one of user compliance. A significant number of users (perhaps a third of them) just don't do what they're told.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 3: Psychology and Usability &gt; Location 3908</div><div class='noteText'>Password guessability ultimately depends on the entropy of the chosen passwords and the number of allowed guesses,</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 3: Psychology and Usability &gt; Location 3910</div><div class='noteText'>Targeted attack on one account: an intruder tries to guess a specific user's password. They might try to guess a rival's logon password at the office, in order to do mischief directly. Attempt to penetrate any account belonging to a specific target: an enemy tries to hack any account you own, anywhere, to get information that might might help take over other accounts, or do harm directly. Attempt to penetrate any account on a target system: the intruder tries to get a logon as any user of the system. This is the classic case of the phisherman trying to hack any account at a target bank so he can launder stolen money through it. Attempt to penetrate any account on any system: the intruder merely wants an account at any system in a given domain but doesn't care which one. Examples are bad guys trying to guess passwords on any online email service so they can send spam from the compromised account, and a targeted attacker who wants a logon to any random machine in the domain of a target company as a beachhead. Attempt to use a breach of one system to penetrate a related one: the intruder has got a beachhead and now wants to move inland to capture higher-value targets. Service-denial attack: the attacker may wish to block one or more legitimate users from using the system. This might be targeted on a particular account or system-wide.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 3: Psychology and Usability &gt; Location 3968</div><div class='noteText'>A trusted path is some means of being sure that you're logging into a genuine machine through a channel that isn't open to eavesdropping.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 3: Psychology and Usability &gt; Location 4023</div><div class='noteText'>The right way to do it is to generate a random key, historically known in this context as a salt; combine the password with the salt using a slow, cryptographically strong one-way function; and store both the salt and the hash.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 3: Psychology and Usability &gt; Location 4063</div><div class='noteText'>if is the maximum password lifetime, is login attempt rate, is the size of the password space, then the probability that a password can be guessed in its lifetime is , according to the US Department of Defense password management guideline</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 4: Protocols &gt; Location 4388</div><div class='noteText'>A nonce can be anything that guarantees the freshness of a message. It can be a random number, a counter, a random challenge received from a third party, or even a timestamp.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 4: Protocols &gt; Location 4392</div><div class='noteText'>Key management in such devices can be very simple. In a typical garage token product, each token's key is just its unique device number encrypted under a global master key known to the garage: This is known as key diversification or key derivation. It's a common way of implementing access tokens, and is widely used in smartcards too. The goal is that someone who compromises a token by drilling into it and extracting the key cannot masquerade as any other token; all he can do is make a copy of one particular subscriber's token. In order to do a complete break of the system, and extract the master key that would enable him to pretend to be any of the system's users, an attacker has to compromise the central server at the garage (which might protect this key in a tamper-resistant smartcard or hardware security module).</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 4: Protocols &gt; Location 4415</div><div class='noteText'>There's the valet attack, where someone with temporary access, such as a valet parking attendant, records some access codes and replays them later to steal your car. In addition, someone might rent a car, record enough unlock codes, and then go back later to the rental lot to steal it.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 4: Protocols &gt; Location 4428</div><div class='noteText'>An example is accessory control. Many printer companies embed authentication mechanisms in printers to ensure that genuine toner cartridges are used. If a competitor's product is loaded instead, the printer may quietly downgrade from 1200 dpi to 300 dpi, or simply refuse to work at all.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 4: Protocols &gt; Location 4731</div><div class='noteText'>In the Internet of Things, keys can sometimes be managed directly and physically, by local setup and a policy of trust-on-first-use or TOFU.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 4: Protocols &gt; Location 4753</div><div class='noteText'>The basic idea behind key-distribution protocols is that where two principals want to communicate, they may use a trusted third party to introduce them.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 4: Protocols &gt; Location 4758</div><div class='noteText'>A simple authentication protocol could run as follows. Alice first calls Sam and asks for a key for communicating with Bob. Sam responds by sending Alice a pair of certificates. Each contains a copy of a key, the first encrypted so only Alice can read it, and the second encrypted so only Bob can read it.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 4: Protocols &gt; Location 4761</div><div class='noteText'>Alice then calls Bob and presents the second certificate as her introduction. Each of them decrypts the appropriate certificate under the key they share with Sam and thereby gets access to the new key. Alice can now use the key to send encrypted messages to Bob, and to receive messages from him in return. We've seen that replay attacks are a known problem, so in order that both Bob and Alice can check that the certificates are fresh, Sam may include a timestamp in each of them. If certificates never expire, there might be serious problems dealing with users whose privileges have been revoked. Using our protocol notation, we could describe this as Expanding the notation, Alice calls Sam and says she'd like to talk to Bob. Sam makes up a message consisting of Alice's name, Bob's name, a session key for them to use, and a timestamp. He encrypts all this under the key he shares with Alice, and he encrypts another copy of it under the key he shares with Bob. He gives both ciphertexts to Alice. Alice retrieves the session key from the ciphertext that was encrypted to her, and passes on to Bob the ciphertext encrypted for him. She now sends him whatever message she wanted to send, encrypted using this session key.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 4: Protocols &gt; Location 4782</div><div class='noteText'>Many existing key distribution protocols are derived from the Needham-Schroeder protocol, which appeared in 1978 [1428]. It is somewhat similar to the above, but uses nonces rather than timestamps. It runs as follows: Message 1 Message 2 Message 3 Message 4 Message 5 Here Alice takes the initiative, and tells Sam: ‘I'm Alice, I want to talk to Bob, and my random nonce is .’ Sam provides her with a session key, encrypted using the key she shares with him. This ciphertext also contains her nonce so she can confirm it's not a replay. He also gives her a certificate to convey this key to Bob. She passes it to Bob, who then does a challenge-response to check that she is present and</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 4: Protocols &gt; Location 4802</div><div class='noteText'>There is a subtle problem with this protocol – Bob has to assume that the key he receives from Sam (via Alice) is fresh. This is not necessarily so: Alice could have waited a year between steps 2 and 3. In many applications this may not be important; it might even help Alice to cache keys against possible server failures. But if an opponent – say Charlie – ever got hold of Alice's key, he could use it to set up session keys with many other principals. And if Alice ever got fired, then Sam had better have a list of everyone in the firm to whom he issued a key for communicating with her, to tell them not to believe it any more. In other words, revocation is a problem: Sam may have to keep complete logs of everything he's ever done, and these logs would grow in size forever unless the principals' names expired at some fixed time in the future.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 4: Protocols &gt; Location 4816</div><div class='noteText'>Kerberos The most important practical derivative of the Needham-Schroeder protocol is Kerberos,</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 4: Protocols &gt; Location 4820</div><div class='noteText'>Instead of a single trusted third party, Kerberos has two kinds: authentication servers to which users log on, and ticket granting servers which give them tickets allowing access to various resources such as files. This enables scalable access management.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 4: Protocols &gt; Location 4824</div><div class='noteText'>First, Alice logs on to the authentication server using a password. The client software in her PC fetches a ticket from this server that is encrypted under her password and that contains a session key . Assuming she gets the password right, she now</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 4: Protocols &gt; Location 4826</div><div class='noteText'>controls and to get access to a resource controlled by the ticket granting server , the following protocol takes place. Its outcome is a key with timestamp and lifetime , which will be used to authenticate Alice's subsequent traffic with that resource: Translating this into English: Alice asks the ticket granting server for access to . If this is permissible, the ticket is created containing a suitable key and given to Alice to use. She also gets a copy of the key in a form readable by her, namely encrypted under . She now verifies the ticket by sending a timestamp to the resource, which confirms it's alive by sending back the timestamp incremented by one (this shows it was able to decrypt the ticket correctly and extract the key ).</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 4: Protocols &gt; Location 4852</div><div class='noteText'>Kerberos is a trusted third-party (TTP) protocol in that is trusted: if the police turn up with a warrant, they can get Sam to turn over the keys and read the traffic.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 4: Protocols &gt; Location 4885</div><div class='noteText'>There are several different approaches to verifying the correctness of protocols. One of the best known is the logic of belief, or BAN logic, named after its inventors Burrows, Abadi and Needham [352]. It reasons about what a principal might reasonably believe having seen certain messages, timestamps and so on.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5036</div><div class='noteText'>There are basically two ways to make a stronger cipher – the stream cipher and the block cipher. In the former, you make the encryption rule depend on a plaintext symbol's position in the stream of plaintext symbols, while in the latter you encrypt several plaintext symbols at once in a block.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5070</div><div class='noteText'>given any ciphertext, and any plaintext of the same length, there's a key that decrypts the ciphertext to the plaintext.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5292</div><div class='noteText'>We've already seen the model of perfect secrecy: given any ciphertext, all possible plaintexts of that length are equally likely.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5294</div><div class='noteText'>an authentication scheme that uses a key only once can be designed so that the best forgery attack on it is a random guess, whose probability of success can be made as low as we want by choosing a long enough tag.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5295</div><div class='noteText'>The second model is concrete security, where we want to know how much actual work an adversary has to do.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5303</div><div class='noteText'>The third model, which many theoreticians now call the standard model, is about indistinguishability. This enables us to reason about the specific properties of a cipher we care about.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5305</div><div class='noteText'>For example, most cipher systems don't hide the length of a message, so we can't define a cipher to be secure by just requiring that an adversary not be able to distinguish ciphertexts corresponding to two messages; we have to be more explicit and require that the adversary not be able to distinguish between two messages and of the same length. This is formalised by having the cryptographer and the cryptanalyst play a game in which the analyst wins by finding an efficient discriminator of something she shouldn't be able to discriminate with more than negligible probability. If the cipher doesn't have perfect security this can be asymptotic, where we typically want the effort to grow faster than any polynomial function of a security parameter – say the length of the key in bits. A security proof typically consists of a reduction where we show that if there exists a randomised (i.e., probabilistic) algorithm running in time polynomial in that learns information it shouldn't with non-negligible probability, then this would give an efficient discriminator for an underlying cryptographic primitive that we already trust. Finally, a construction is said to have semantic security if there's no efficient distinguisher for the plaintext regardless of any side information the analyst may have about it; even if she knows all but one bit of it, and even if she can get a decryption of any other ciphertext, she can't learn anything more from the target ciphertext. This skips over quite a few mathematical details, which you can find in a standard text such as Katz and Lindell</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5319</div><div class='noteText'>The fourth model is the random oracle model, which is not as general as the standard model but which often leads to more efficient constructions.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5334</div><div class='noteText'>The first type of random oracle is the random function. A random function accepts an input string of any length and outputs a string of fixed length, say bits long. The same input gives the same output, but the set of outputs appears random.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5346</div><div class='noteText'>The first main property of a random function is one-wayness. Given knowledge of an input we can easily compute the hash value , but it is very difficult given to find if such an input is not already known.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5350</div><div class='noteText'>As the output is random, the best an attacker can do to invert a random function is to keep on feeding in more inputs until he gets lucky; with an -bit output this will take about guesses on average.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5351</div><div class='noteText'>A pseudorandom function will have the same properties, or they could be used to distinguish it from a random function, contrary to our definition. So a pseudorandom function will also be a one-way function, provided there are too many possible outputs for the opponent to guess an input that has a desired target output by chance. This means choosing so that the opponent can't do anything near computations.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5357</div><div class='noteText'>A second property of pseudorandom functions is that the output will not give any information at all about even part of the input. So we can get a one-way encryption of the value by concatenating it with a secret key and computing .</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5363</div><div class='noteText'>A third property of pseudorandom functions with sufficiently long outputs is that it is hard to find collisions, that is, different messages with .</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5413</div><div class='noteText'>The second basic cryptographic primitive is the random generator, also known as a keystream generator or stream cipher. This is also a random function, but it's the reverse of the hash function in that it has a short input and a long output.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5419</div><div class='noteText'>It can be used to protect the confidentiality of our backup data as follows: we go to the keystream generator, enter a key, get a long file of random bits, and exclusive-or it with our plaintext data to get ciphertext, which we then send to our backup service in the cloud. (This is also called an additive stream cipher as exclusive-or is addition modulo 2.)</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5428</div><div class='noteText'>One-time pad systems are a close fit for our theoretical model, except in that they are used to secure communications across space rather than time: the two communicating parties have shared a copy of a keystream in advance.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5432</div><div class='noteText'>A real problem with keystream generators is to prevent the same keystream being used more than once, whether to encrypt more than one backup tape or to encrypt more than one message sent on a communications channel.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5440</div><div class='noteText'>the normal engineering practice is to have not just a key but also a seed (also known as an initialisation vector or IV)</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5446</div><div class='noteText'>The third type of primitive, and the most important in modern cryptography, is the block cipher, which we model as a random permutation.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5457</div><div class='noteText'>A block cipher is a keyed family of pseudorandom permutations. For each key, we have a single permutation that's independent of all the others.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5462</div><div class='noteText'>We will write a block cipher using the notation established for encryption in the chapter on protocols: The random permutation model also allows us to define different types of attack on block ciphers. In a known plaintext attack, the opponent is just given a number of randomly chosen inputs and outputs from the oracle corresponding to a target key. In a chosen plaintext attack, the opponent is allowed to put a certain number of plaintext queries and get the corresponding ciphertexts. In a chosen ciphertext attack he gets to make a number of ciphertext queries. In a chosen plaintext/ ciphertext attack he is allowed to make queries of either type. Finally, in a related key attack he can make queries that will be answered using keys related to the target key , such as and .</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5472</div><div class='noteText'>the objective of the attacker may be either to deduce the answer to a query he hasn't already made (a forgery attack), or to recover the key (unsurprisingly known as a key recovery attack).</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5477</div><div class='noteText'>the first major attack announced on the Data Encryption Standard algorithm (differential cryptanalysis) required chosen plaintexts to recover the key, while the next major attack (linear cryptanalysis) improved this to known plaintexts.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5492</div><div class='noteText'>Chosen plaintext/ ciphertext attacks may be a worry where the threat is a lunchtime attack: someone who gets temporary access to a cryptographic device while its authorised user is out, and tries out the full range of permitted operations for a while with data of their choice. Related-key attacks are a concern where the block cipher is used as a building block in the construction of a hash function (which we'll discuss below). To exclude all such attacks, the goal is semantic security, as discussed above; the cipher should not allow the inference of unauthorised information (whether of plaintexts, ciphertexts or keys) other than with negligible probability.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5505</div><div class='noteText'>The simplest variation is the trapdoor one-way permutation. This is a computation that anyone can perform, but which can be reversed only by someone who knows a trapdoor such as a secret key.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5526</div><div class='noteText'>The final cryptographic primitive we'll define here is the digital signature. The basic idea is that a signature on a message can be created by only one principal, but checked by anyone.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5568</div><div class='noteText'>He described the properties of a cipher as being confusion and diffusion – adding unknown key values will confuse an attacker about the value of a plaintext symbol, while diffusion means spreading the plaintext information through the ciphertext. Block ciphers need diffusion as well as confusion.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5587</div><div class='noteText'>Three things need to be done to make such a design secure: the cipher needs to be “wide” enough it needs to have enough rounds, and the S-boxes need to be suitably chosen.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5617</div><div class='noteText'>Linear cryptanalysis [897, 1246] proceeds by collecting a number of relations such as “bit 2 plus bit 5 of the input to the first S-box is equal to bit 1 plus bit 8 of the output, with probability 13/ 16”, then searching for ways to glue them together into an algebraic relation between input bits, output bits and key bits that holds with a probability different from one half. If we can find a linear relationship that holds over the whole cipher with probability , then according to the sampling theorem in probability theory we can expect to start recovering keybits once we have about known texts. If the value of for the best linear relationship is greater than the total possible number of known texts (namely where the inputs and outputs are bits wide), then we consider the cipher to be secure against linear cryptanalysis.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5655</div><div class='noteText'>The Advanced Encryption Standard (AES) is an algorithm originally known as Rijndael after its inventors Vincent Rijmen and Joan Daemen [507]. It acts on 128-bit blocks and can use a key of 128, 192 or 256 bits in length. It is an SP-network; in order to specify it, we need to fix the S-boxes, the linear transformation between the rounds, and the way in which the key is added into the computation.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5671</div><div class='noteText'>16 bytes of key material are needed per round; they are derived from the user supplied key material by means of a recurrence relation.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5678</div><div class='noteText'>The algorithm uses 10 rounds with 128-bit keys, 12 rounds with 192-bit keys and 14 rounds with 256-bit keys.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5679</div><div class='noteText'>These are enough to give practical, but not certificational, security – as indeed we expected at the time of the AES competition, and as I described in earlier editions of this chapter. The first key-recovery attacks use a technique called biclique cryptanalysis and were discovered in 2009 by Andrey Bogdanov, Dmitry Khovratovich, and Christian Rechberger [274]; they give only a very small advantage, with complexity now estimated at for 128-bit AES and for 256-bit AES, as opposed to and for brute-force search. Faster shortcut attacks are known for the case where we have related keys.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5698</div><div class='noteText'>Practical attacks include timing analysis and power analysis. In the former, the main risk is that an opponent observes cache misses and uses them to work out the key. In the latter, an opponent uses measurements of the current drawn by the device doing the crypto</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5701</div><div class='noteText'>Emission Security; countermeasures include special operations in many CPUs to do AES, which are available precisely because the algorithm is now a standard. It does not make sense to implement Serpent as well, ‘just in case AES is broken’: having swappable algorithms is known as pluggable cryptography, yet the risk of a fatal error in the algorithm negotiation protocol is orders of magnitude greater than the risk that anyone will come up with a production attack on AES.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5720</div><div class='noteText'>A Feistel cipher has the ladder structure shown in Figure 5.12. The input is split up into two blocks, the left half and the right half. A round function of the left half is computed and combined with the right half using exclusive-or (binary addition without carry), though in some Feistel ciphers addition with carry is also used.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5737</div><div class='noteText'>we just use the round functions in the reverse order. Thus the round functions do not have to be invertible, and the Feistel structure lets us turn any one-way function into a block cipher.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5751</div><div class='noteText'>The DES algorithm is widely used in banking and other payment applications. The ‘killer app’ that got it widely deployed was ATM networks; from there it spread to prepayment meters, transport tickets and much else. In its classic form, it is a Feistel cipher, with a 64-bit block and 56-bit key. Its round function operates on 32-bit half blocks and consists of three operations: first, the block is expanded from 32 bits to 48; next, 48 bits of round key are mixed in using exclusive-or; the result is passed through a row of eight S-boxes, each of which takes a six-bit input and provides a four-bit output; finally, the bits of the output are permuted according to a fixed pattern. The effect of the expansion, key mixing and S-boxes is shown in Figure 5.13: Figure 5.13: The DES round function</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5766</div><div class='noteText'>The most telling criticism was that the key is too short. Someone who wants to find a 56 bit key using brute force, that is by trying all possible keys, will have a total exhaust time of encryptions and an average solution time of half that, namely encryptions.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5782</div><div class='noteText'>Another criticism of DES was that, since IBM kept its design principles secret at the request of the US government, perhaps there was a ‘trapdoor’ which would give them easy access.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5787</div><div class='noteText'>The best known shortcut attack, that is, a cryptanalytic attack involving less computation than keysearch, is a linear attack using known texts.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5825</div><div class='noteText'>In electronic code book mode, we just encrypt each succeeding block of plaintext with our block cipher to get ciphertext, as with the Playfair example above. This is adequate for protocols using single blocks such as challenge-response and some key management tasks; it's also used to encrypt PINs in cash machine systems. But if we use it to encrypt redundant data the patterns will show through, giving an opponent information about the plaintext.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5845</div><div class='noteText'>Most commercial applications which encrypt more than one block used to use cipher block chaining, or CBC, mode. Like ECB, this was one of the original modes of operation standardised with DES. In it, we exclusive-or the previous block of ciphertext to the current block of plaintext before encryption (see Figure 5.15). This mode disguises patterns in the plaintext: the encryption of each block depends on all the previous blocks. The input initialisation vector (IV) ensures that stereotyped plaintext message headers won't leak information by encrypting to identical ciphertexts, just as with a stream cipher.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5857</div><div class='noteText'>padding oracle attack in which the attacker tweaks input ciphertexts, one byte at a time, watches the error messages, and ends up being able to decrypt whole messages.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5874</div><div class='noteText'>Additive stream ciphers have two systemic vulnerabilities, as we noted in section 5.2.2 above. The first is an attack in depth: if the same keystream is used twice, then the xor of the two ciphertexts is the xor of the two plaintexts, from which plaintext can often be deduced, as with Venona. The second is that they fail to protect message integrity.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5888</div><div class='noteText'>Output feedback mode consists of repeatedly encrypting an initial value and using this as a keystream in a stream cipher. Writing IV for the initialization vector, we will have and . However an -bit block cipher in OFB mode will typically have a cycle length of blocks, after which the birthday theorem will see to it that we loop back to the IV.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5898</div><div class='noteText'>Cipher feedback mode is another kind of stream cipher, designed for use in radio systems that have to resist jamming. It was designed to be self-synchronizing, in that even if we get a burst error and drop a few bits, the system will recover synchronization after one block length. This is achieved by using our block cipher to encrypt the last bits of ciphertext, adding the last output bit to the next plaintext bit, and shifting the ciphertext along one bit. But this costs one block cipher operation per bit and has very bad error amplification properties; nowadays people tend to use dedicated link layer protocols for synchronization and error correction rather than trying to combine them with the cryptography at the traffic layer.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5925</div><div class='noteText'>Galois Counter Mode (GCM) has taken over as the default since being approved by NIST in 2007 [1409]. It uses only one invocation of the block cipher per block of text, and it's parallelisable so you can get high throughput on fast data links with low cost and low latency. Encryption is performed in a variant of counter mode; the resulting ciphertexts are also used as coefficients of a polynomial which is evaluated at a key-dependent point over a Galois field of elements to give an authenticator tag.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5940</div><div class='noteText'>GCM and other authenticated encryption modes expand the plaintext by adding a message key and an authenticator tag. This is very inconvenient in applications such as hard disk encryption, where we prefer a mode of operation that preserves plaintext length. Disk encryption systems used to use CBC with the sector number providing an IV, but since Windows 10, Microsoft has been using a new mode of operation, XTS-AES, inspired by GCM and standardised in 2007. This is a codebook mode but with the plaintext whitened by a tweak key derived from the disk sector. Formally, the message encrypted with the key at block is where the tweak key is derived by encrypting the IV using a different key and then multiplying it repeatedly with a suitable constant so as to give a different whitener for each block. This means that if an attacker swaps two encrypted blocks, all 256 bits will decrypt to randomly wrong values. You still need higher-layer mechanisms to detect ciphertext manipulation, but simple checksums will be sufficient.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 5975</div><div class='noteText'>MD4 has three rounds and a 128 bit hash value, and a collision was found for it in 1998 [568]; MD5 has four rounds and a 128 bit hash value, and a collision was found for it in 2004 [1983, 1985]; SHA-1, released in 1995, has five rounds and a 160 bit hash value. A collision was found in 2017 [1831], and a more powerful version of the attack in 2020 [1148]; SHA-2, which replaced it in 2002, comes in 256-bit and 512-bit versions (called SHA256 and SHA512) plus a number of variants.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6014</div><div class='noteText'>Hash functions have many other uses. One of them is to compute MACs. A naïve method would be to hash the message with a key: MAC. However the accepted way of doing this, called HMAC, uses an extra step in which the result of this computation is hashed again. The two hashing operations are done using variants of the key, derived by exclusive-or'ing them with two different constants. Thus HMAC. is constructed by repeating the byte 0x36 as often as necessary, and similarly from the byte 0x5C. If a hash function is on the weak side, this construction can make exploitable collisions harder to find [1091]. HMAC is now FIPS 198-1.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6028</div><div class='noteText'>Merkle trees hash a large number of inputs to a single hash output. The inputs are hashed to values that form the leaves of a tree; each non-leaf node contains the hash of all the hashes at its child nodes, so the hash at the root is a hash of all the values at the leaves. This is a fast way to hash a large data structure; it's used in code signing, where you may not want to wait for all of an application's files to have their signatures checked before you open it. It's also widely used in blockchain applications; in fact, a blockchain is just a Merkle tree. It was invented by Ralph Merkle, who first proposed it to calculate a short hash of a large file of public keys [1298], particularly for systems where public keys are used only once.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6042</div><div class='noteText'>Key updating means that two or more principals who share a key pass it through a one-way hash function at agreed times:</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6045</div><div class='noteText'>The chain of compromise is broken by the hash function's one-wayness. This property is also known as backward security.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6046</div><div class='noteText'>A variant is autokeying where the principals update a key by hashing it with the messages they have exchanged since the last key change: . If an attacker now compromises one of their systems and steals the key, then as soon as they exchange a message which he can't observe or guess, security will be recovered; again, the chain of compromise is broken. This property is known as forward security.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6063</div><div class='noteText'>The prime numbers are the positive whole numbers with no proper divisors: the only numbers that divide a prime number are 1 and the number itself.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6077</div><div class='noteText'>The algorithm commonly used to do public-key encryption and digital signatures based on factoring is RSA, named after its inventors Ron Rivest, Adi Shamir and Len Adleman. It uses Fermat's little theorem, which states that for all primes not dividing , (mod ) (proof: take the set {1, 2, …, } and multiply each of them modulo by , then cancel out each side). For a general integer , (mod ) where Euler's function is the number of positive integers less than with which it has no divisor in common (the proof is similar). So if is the product of two primes then . In RSA, the encryption key is a modulus which is hard to factor (take for two large randomly chosen primes and , say of 1024 bits each) plus a public exponent that has no common factors with either or . The private key is the factors and , which are kept secret. Where is the message and is the ciphertext, encryption is defined by Decryption is the reverse operation: Whoever knows the private key – the factors and of – can easily calculate . As and has no common factors with , the key's owner can find a number such that – she finds the value of separately modulo and , and combines the answers. (mod ) is now computed as (mod ), and decryption works because of Fermat's theorem:</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6109</div><div class='noteText'>the owner of a private key can operate on a message with it to produce a signature and this signature can be verified by raising it to the power mod (thus, using and as the public signature verification key) and checking that the message is recovered: Neither RSA encryption nor signature is safe to use on its own. The reason is that, as encryption is an algebraic process, it preserves certain algebraic properties. For example, if we have a relation such as that holds among plaintexts, then the same relationship will hold among ciphertexts and signatures . This property is known as a multiplicative homomorphism; a homomorphism is a function that preserves some mathematical structure. The homomorphic nature of raw RSA means that it doesn't meet the random oracle model definitions of public key encryption or signature.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6122</div><div class='noteText'>Another general problem with public-key encryption is that if the plaintexts are drawn from a small set, such as ‘attack’ or ‘retreat’, and the encryption process is deterministic (as RSA is), then an attacker might just precompute the possible ciphertexts and recognise them when they appear. With RSA, it's also dangerous to use a small exponent to encrypt the same message to multiple recipients, as this can lead to an algebraic attack. To stop the guessing attack, the low-exponent attack and attacks based on homomorphism, it's sensible to add in some randomness, and some redundancy, into a plaintext block before encrypting it. Every time we encrypt the same short message, say ‘attack’, we want to get a completely different ciphertext, and for these to be indistinguishable from each other as well as from the ciphertexts for ‘retreat’.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6137</div><div class='noteText'>The usual real-world solution is optimal asymmetric</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6137</div><div class='noteText'>encryption padding (OAEP), where we concatenate the message with a random nonce , and use a hash function to combine them: In effect, this is a two-round Feistel cipher that uses as its round function. The result, the combination , is then encrypted with RSA and sent. The recipient then computes as and recovers as [213]. This was eventually proven to be secure. There are a number of public-key cryptography standards; PKCS #1 describes OAEP [995].</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6172</div><div class='noteText'>A primitive root modulo is a number whose powers generate all the nonzero numbers mod ; for example, when working modulo 7 we find that = 25 which reduces to 4 (modulo 7), then we can compute as or which is 20, which reduces to 6 (modulo 7), and so on, as in Figure 5.17. Thus 5 is a primitive root modulo 7. This means that given any , we can always solve the equation (mod 7); is then called the discrete logarithm of modulo 7. Small examples like this can be solved by inspection, but for a large random prime number , we do not know how to do this efficiently. So the mapping (mod ) is a one-way function, with the additional properties that and . In other words, it is a one-way homomorphism. As such, it can be used to construct digital signature and public key encryption algorithms.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6233</div><div class='noteText'>5.7.2.2 Diffie-Hellman key establishment The first public-key encryption scheme to be published, by Whitfield Diffie and Martin Hellman in 1976, has a fixed primitive root and uses modulo as the key to a shared-key encryption system. The values and can be the private keys of the two parties. Let's walk through this. The prime and generator are common to all users. Alice chooses a secret random number , calculates and publishes it opposite her name in the company phone book. Bob does the same, choosing a random number and publishing . In order to communicate with Bob, Alice fetches from the phone book, forms which is just , and uses this to encrypt the message to Bob. On receiving it, Bob looks up Alice's public key and forms which is also</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6248</div><div class='noteText'>common to all users. Alice chooses a random number , calculates and sends it to Bob; Bob does the same, choosing a random number and sending to Alice; they then both form , which they use as a session key (see Figure 5.18). Alice and Bob can now use the session key to encrypt a conversation. If they used transient keys, rather than long-lived ones, they have managed to create a shared secret ‘out of nothing’. Even if an opponent had inspected both their machines before this protocol was started, and knew all their stored private keys, then provided some basic conditions were met (e.g., that their random number generators were not predictable and no malware was left behind) the opponent could still not eavesdrop on their traffic. This is the strong version of the forward security property to which I referred in section 5.6.2. The opponent can't work forward from knowledge of previous keys, however it was obtained. Provided that Alice and Bob both destroy the shared secret after use, they will also have backward security: an opponent who gets access to their equipment later cannot work backward to break their old traffic. In what follows, we may write the Diffie-Hellman key derived from and as when we don't have to be explicit about which group we're working in, and don't need to write out explicitly which is the private key and which is the public key . Figure 5.18: The Diffie-Hellman key</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6310</div><div class='noteText'>Suppose that the base and the generator are public values chosen in some suitable way, and that each user who wishes to sign messages has a private signing key with a public signature verification key . An ElGamal signature scheme works as follows. Choose a message key at random, and form (mod ). Now form the signature using a linear equation in , , the message and the private key . There are a number of equations that will do; the one that happens to be used in ElGamal signatures is So is computed as ; this is done modulo . When both sides are passed through our one-way homomorphism mod we get: or An ElGamal signature on the message consists of the values and , and the recipient can verify it using the above equation.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6329</div><div class='noteText'>Digital Signature Algorithm (DSA) which is a US standard and widely used in government applications. DSA assumes a prime of typically 2048 bits7, a prime of 256 bits dividing , an element of order in the integers modulo , a secret signing key and a public verification key . The signature on a message , , is where The hash function used by default is SHA2568. DSA is the classic example of a randomised digital signature scheme without message recovery. The most commonly-used version nowadays is ECDSA,</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6347</div><div class='noteText'>Elliptic curve cryptography uses discrete logarithms on an elliptic curve – a curve given by an equation like . These curves have the property that you can define an addition operation on them and the resulting Mordell group can be used for cryptography.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6353</div><div class='noteText'>First is performance; they give versions of the familiar primitives such as Diffie-Hellmann key exchange and the Digital Signature Algorithm that use less computation, and also have shorter variables; both are welcome in constrained environments. Elliptic curve cryptography is used in applications from the latest versions of EMV payment cards to Bitcoin.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6356</div><div class='noteText'>some elliptic curves have a bilinear pairing which Dan Boneh and Matt Franklin used to construct cryptosystems where your public key is your name</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6357</div><div class='noteText'>Recall that in RSA and Diffie-Hellmann, the user chose his private key and then computed a corresponding public key. In a so-called identity-based cryptosystem, you choose your identity then go to a central authority that issues you with a private key corresponding to that identity. There is a global public key, with which anyone can encrypt a message to your identity; you can decrypt this using your private key. Earlier, Adi Shamir had discovered identity-based signature schemes that allow you to sign messages using a private key so that anyone can verify the signature against your name [1707]. In both cases, your private key is computed by the central authority using a system-wide private key known only to itself.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6373</div><div class='noteText'>The CA might be run by the local system administrator; but it is most commonly a third party service such as Verisign whose business is to sign public keys after doing some due diligence about whether they are controlled by the principals named in them. A certificate might be described symbolically as (5.1) where is the certificate's starting date and time, is the length of time for which it is valid, A is the user's name, is her public encryption key, and is her public signature verification key. In this way, only the administrator's public signature verification key needs to be communicated to all principals in a trustworthy manner.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6401</div><div class='noteText'>Here is a simplified description of the basic version of the protocol in TLS v1: the client sends the server a client hello message that contains its name , a transaction serial</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6403</div><div class='noteText'>number , and a random nonce ; the server replies with a server hello message that contains its name , a transaction serial number , a random nonce , and a certificate containing its public key . The client now checks the certificate , and if need be checks the key that signed it in another certificate, and so on back to a root certificate issued by a company such as Verisign and stored in the browser; the client sends a key exchange message containing a pre-master-secret key, , encrypted under the server public key . It also sends a finished message with a message authentication code (MAC) computed on all the messages to date. The key for this MAC is the master-secret, . This key is computed by hashing the pre-master-secret key with the nonces sent by the client and server: . From this point onward, all the traffic is encrypted; we'll write this as in the client-server direction and from the server to the client. These keys are generated in turn by hashing the nonces with . The server also sends a finished message with a MAC computed on all the messages to date. It then finally starts sending the data. Once a client and server have established a pre-master-secret, no more public-key operations are needed as further master secrets can be obtained by hashing it with new nonces.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6474</div><div class='noteText'>Code signing was introduced in the 1990s when people started downloading software rather than getting it on diskettes. It is now used very widely to assure the provenance of software. You might think that having a public signature-verification key in your software so that version can verify an update to version would be a simple application of public-key cryptography but this is far from the case.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6505</div><div class='noteText'>QUIC is a new UDP-based protocol designed by Google and promoted as an alternative to TLS that allows quicker session establishment and cutting latency in the ad auctions that happen as pages load; sessions can persist as people move between access points. This is achieved by a cookie that holds the client's last IP address, encrypted by the server. It appeared in Chrome in 2013 and now has about 7% of Internet traffic; it's acquired a vigorous standardisation community. Google claims it reduces search latency 8% and YouTube buffer time 18%.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6518</div><div class='noteText'>Threshold crypto is a mechanism whereby a signing key, or a decryption key, can be split up among principals so that any out of can sign a message (or decrypt). For the construction is easy. With RSA, for example, you can split up the private key as . For it's</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 5: Cryptography &gt; Location 6522</div><div class='noteText'>slightly more complex (but not much – you use the Lagrange interpolation formula) [554]. Threshold signatures were first used in systems where a number of servers process transactions independently and vote independently on the outcome; they have more recently been used to implement business rules on cryptocurrency wallets such as ‘a payment must be authorised by any two of the seven company directors’.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 6770</div><div class='noteText'>They are often described as discretionary access control (DAC) mechanisms, which leave protection to the machine operator, or mandatory access control (MAC) mechanisms which are typically under the control of the vendor and protect the operating system itself from</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 6772</div><div class='noteText'>being modified by malware.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 6825</div><div class='noteText'>Another way of expressing a policy of this type would be with access triples of (user, program, file). In the general case, our concern isn't with a program so much as a protection domain which is a set of processes or threads that share access to the same resources.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 6867</div><div class='noteText'>ACLs have a number of advantages and disadvantages as a means of managing security state. They are a natural choice in environments where users manage their own file security, and became widespread in Unix systems from the 1970s. They are the basic access control mechanism in Unix-based systems such as Linux and Apple's macOS, as well as in derivatives such as Android and iOS. The access controls in Windows were also based on ACLs, but have become more complex over time. Where access control policy is set centrally, ACLs are suited to environments where protection is data-oriented; they are less suited where the user population is large and constantly changing, or where users want to be able to delegate their authority to run a particular program to another user for some set period of time.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 6905</div><div class='noteText'>Second, ACLs only contain the names of users, not of programs; so there is no straightforward way to implement access triples of (user, program, file).</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 6923</div><div class='noteText'>Fourth, the Unix ACL only names one user. If a resource will be used by more than one of them, and you want to do access control at the OS level, you have a couple of options. With older systems you had to use groups; newer systems implement the Posix system of extended ACLs, which may contain any number of named user and named group entities.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7064</div><div class='noteText'>First, rather than just read, write and execute there were separate attributes for take ownership, change permissions and delete, to support more flexible delegation.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7073</div><div class='noteText'>Second, users and resources can be partitioned into domains with distinct administrators, and trust can be inherited between domains in one direction or both. In a typical large company, you might put all the users into a personnel domain administered by HR, while assets such as servers and printers may be in resource domains under departmental control; individual workstations may even be administered by their users. Things can be arranged so that the departmental resource domains trust the user domain, but not vice versa – so a hacked or careless departmental administrator can't do too much external damage. The individual workstations would in turn trust the department (but not vice versa) so that users can perform tasks that require local privilege (such as installing software packages). Limiting the damage a hacked administrator can do still needs careful organisation. The data structure used to manage all this, and hide the ACL details from the user interface, is called the Registry. Its core used to be the Active Directory, which managed remote authentication – using either a Kerberos variant or TLS, encapsulated behind the Security Support Provider Interface (SSPI), which enables administrators to plug in other authentication services. Active Directory is essentially a database that organises users, groups, machines, and organisational units within a domain in a hierarchical namespace. It lurked behind Exchange, but is now being phased out as Microsoft becomes a cloud-based company and moves its users to Office365.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7085</div><div class='noteText'>Windows has added capabilities in two ways which can override or complement ACLs. First, users or groups can be either allowed or denied access by means of profiles. Security policy is set by groups rather than for the system as a whole; group policy overrides individual profiles, and can be associated with sites, domains or organisational units, so it can start to tackle complex problems. Policies can be created using standard tools or custom coded. The second way in which capabilities insinuate their way into Windows is that in many applications, people use TLS for authentication, and TLS certificates provide another, capability-oriented, layer of access control</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7090</div><div class='noteText'>outside the purview of the Active Directory.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7091</div><div class='noteText'>I already mentioned that Windows Vista introduced trusted boot to make the operating system itself tamper-resistant, in the sense that it always boots into a known state, limiting the persistence of malware. It added three further protection mechanisms to get away from the previous default of all software running as root. First, the kernel was closed off to developers; second, the graphics subsystem and most drivers were removed from the kernel; and third, User Account Control (UAC) replaced the default administrator privilege with user defaults instead. Previously, so many routine tasks needed administrative privilege that many enterprises made all their users administrators, which made it difficult to contain malware; and many developers wrote their software on the assumption that it would have access to everything. According to Microsoft engineers, this was a major reason for Windows' lack of robustness: applications monkey with system resources in incompatible ways. So they added an Application Information Service that launches applications which require elevated privilege and uses virtualisation to contain them: if they modify the registry, for example, they don't modify the ‘real’ registry but simply the version of it that they can see.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7100</div><div class='noteText'>Since Vista, the desktop acts as the parent process for later user processes, so even administrators browse the web as normal users, and malware they download can't overwrite system files unless given later authorisation. When a task requires admin privilege, the user gets an elevation prompt asking them for an admin password. (Apple's macOS is similar although the details under the hood differ somewhat.) As admin users are often tricked into installing malicious software, Vista added mandatory access controls in the form of file integrity levels. The basic idea is that low-integrity processes (such as code you download from the Internet) should not be able to modify high-integrity data (such as system files) in the absence of some trusted process (such as verification of a signature by Microsoft on the code in question).</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7106</div><div class='noteText'>In 2012, Windows 8 added dynamic access control which lets you control user access by context, such as their work PC versus their home PC and their phone; this is done via account attributes in Active Directory, which appear as claims about a user, or in Kerberos tickets as claims about a domain. In 2016, Windows 8.1 added a cleaner abstraction with principals, which can be a user, computer, process or thread running in a security context or a group to which such a principal belongs, and security identifiers (SIDs), which represent such principals. When a user signs in, they get tickets with the SIDs to which they belong. Windows 8.1 also prepared for the move to cloud computing by adding Microsoft accounts (formerly LiveID), whereby a user signs in to a Microsoft cloud service rather than to a local server. Where credentials are stored locally, it protects them using virtualisation. Finally, Windows 10 added a number of features to support the move to</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7155</div><div class='noteText'>The web browser is another middleware platform on which we rely for access control and whose complexity often lets us down. The main access control rule is the same-origin policy whereby JavaScript or other active content on a web page is only allowed to communicate with the IP address that it originally came from; such code is run in a sandbox to prevent it altering the host system,</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7177</div><div class='noteText'>Bugs in browsers are exploited in drive-by download attacks, where visiting an attack web page can infect your machine, and even without this the modern web environment is extremely difficult to control. Many web pages are full of trackers and other bad things, supplied by multiple ad networks and data brokers, which make a mockery of the intent behind the same-origin policy. Malicious actors can even use web services to launder origin: for example, the attacker makes a mash-up of the target site plus some evil scripts of his own, and then gets the victim to view it through a proxy such as Google Translate. A prudent person will go to their bank website by typing in the URL directly, or using a bookmark; unfortunately, the marketing industry trains everyone to click on links in emails.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7185</div><div class='noteText'>The late 1990s saw the emergence of yet another type of access control: the software sandbox, introduced by Sun with its Java programming language. The model is that a user wants to run some code that she has downloaded as an applet, but is concerned that the applet might do something nasty, such as stealing her address book and mailing it off to a marketing company, or just hogging the CPU and running down the battery. The designers of Java tackled this problem by providing a ‘sandbox’ – a restricted environment in which the code has no access to the local hard disk (or at most only temporary access to a restricted directory), and is only allowed to communicate with the host it</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7190</div><div class='noteText'>came from (the same-origin policy). This is enforced by having the code executed by an interpreter – the Java Virtual Machine (JVM) – with only limited access rights [784]. This idea was adapted to JavaScript, the main scripting language used in web pages, though it's actually a different language; and other active content too. A version of Java is also used on smartcards so they can support applets written by different firms.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7248</div><div class='noteText'>Preventing one process from interfering with another is the protection problem. The confinement problem is that of preventing programs communicating outward other than through authorized channels. There are several flavours of each. The goal may be to prevent active interference, such as memory overwriting, or to stop one process reading another's memory directly. This is what commercial operating systems set out to do. Military systems may also try to protect metadata – data about other data, or subjects, or processes – so that, for example, a user can't find out what other users are logged on to the system or what processes they're running.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7257</div><div class='noteText'>This usually means that hardware access control must be integrated with the processor's memory management functions. A classic mechanism is segment addressing. Memory is addressed by two registers, a segment register that points to a segment of memory, and an address register that points to a location within that segment. The segment registers are controlled by the operating system, often by a component of it called the reference monitor which links the access control mechanisms with the hardware.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7364</div><div class='noteText'>The user/ root distinction became less important in the twenty-first century for two reasons. First, Windows PCs were the most common online devices (until 2017 when Android overtook them) so they were the most common attack targets; and as they ran many applications as administrator, an application that could be compromised typically gave administrator access. Second, attackers come in two basic types: targeted attackers, who want to spy on a specific individual and whose goal is typically to acquire access to that person's accounts; and scale attackers, whose goal is typically to compromise large numbers of PCs, which they can organise into a botnet. This, too, doesn't require administrator access. Even if your mail client does not run as administrator, it can still be used by a spammer who takes control. However, botnet herders do prefer to install rootkits which, as their name suggests, run as root; they are also known as remote access trojans or RATs. The user/ root distinction does still matter in business environments, where you do not want such a kit installed as an advanced persistent threat by a hostile intelligence agency, or by a corporate espionage firm, or by a crime gang doing reconnaissance to set you up for a large fraud.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7375</div><div class='noteText'>A separate distinction is whether an exploit is wormable – whether it can be used to spread malware quickly online from one machine to another without human intervention. The Morris worm was the first large-scale case of this, and there have been many since. I mentioned Wannacry and NotPetya in chapter 2; these used a vulnerability developed by the NSA and then leaked to other state actors. Operating system vendors react quickly to wormable exploits, typically releasing out-of-sequence patches, because of the scale of the damage they can do. The most troublesome wormable exploits at the time of writing are variants of Mirai, a worm used to take over IoT devices that use known root passwords. This appeared in October 2016 to exploit CCTV cameras, and hundreds of versions have been produced since, adapted to take over different vulnerable devices and recruit them into botnets. Wormable exploits often use root access but don't have to; it is sufficient that the exploit be capable of automatic onward transmission9. I will discuss the different</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7387</div><div class='noteText'>The classic software exploit is the memory overwriting attack, colloquially known as ‘smashing the stack’, as used by the Morris worm in 1988; this infected so many Unix machines that it disrupted the Internet and brought malware forcefully to the attention of the mass media [1810]. Attacks involving violations of memory safety accounted for well over half the exploits</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7390</div><div class='noteText'>against operating systems in the late 1990s and early 2000s [484] but the proportion has been dropping slowly since then. Programmers are often careless about checking the size of arguments, so an attacker who passes a long argument to a program may find that some of it gets treated as code rather than data. The classic example, used in the Morris worm, was a vulnerability in the Unix finger command. A common implementation of this would accept an argument of any length, although only 256 bytes had been allocated for this argument by the program. When an attacker used the command with a longer argument, the trailing bytes of the argument ended up overwriting the stack and being executed by the system. The usual exploit technique was to arrange for the trailing bytes of the argument to have a landing pad – a long space of no-operation (NOP) commands, or other register commands that didn't change the control flow, and whose task was to catch the processor if it executed any of them. The landing pad delivered the processor to the attack code which will do something like creating a shell with administrative privilege directly (see Figure 6.5). Stack-overwriting attacks were around long before 1988. Most of the early 1960s time-sharing systems suffered from this vulnerability, and fixed it [805]. Penetration testing in the early '70s showed that one of the most frequently-used attack strategies was still “unexpected parameters” [1168]. Intel's 80286 processor introduced explicit parameter checking instructions – verify read, verify write, and verify length – in 1982, but they were avoided by most software designers to prevent architecture dependencies. Stack overwriting attacks have been found against all sorts of programmable devices – even against things like smartcards and hardware security modules, whose designers really should have known better.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7413</div><div class='noteText'>Many vulnerabilities are variations on the same general theme, in that they occur when data in grammar A is interpreted as being code in grammar B. A stack overflow is when data are accepted as input (e.g. a URL) and end up being executed as machine code. These are failures of type safety. In fact, a stack overflow can be seen either as a memory safety failure or as a failure to sanitise user input, but there are purer examples of each type.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7417</div><div class='noteText'>The use after free type of safety failure is now the most common cause of remote execution vulnerabilities and has provided a lot of attacks on browsers in recent years. It can happen when a chunk of memory is freed and then still used, perhaps because of confusion over which part of a program is responsible for freeing it. If a malicious chunk is now allocated, it may end up taking its place on the heap, and when an old innocuous function is called a new, malicious function may be invoked instead. There are many other variants on the memory safety theme; buffer overflows can be induced by improper string termination, passing an inadequately sized buffer to a path manipulation function, and many other subtle errors. See Gary McGraw's book ‘Software Security [1268] for a taxonomy.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7430</div><div class='noteText'>Once such type-safety and input-sanitisation attacks are dealt with, race conditions are probably next. These occur when a transaction is carried out in two or more stages, where access rights are verified at the first stage and something sensitive is done at the second. If someone can alter the state in between the two stages, this can lead to an attack. A classic example arose in early versions of Unix, where the command to create a directory, ‘mkdir’, used to work in two steps: the storage was allocated, and then ownership was transferred to the user. Since these steps were separate, a user could initiate a ‘mkdir’ in background, and if this completed only the first step before being suspended, a second process could be used to replace the newly created directory with a link to the password file. Then the original process would resume, and change ownership of the password file to the user.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7450</div><div class='noteText'>One attack that has attracted a lot of research effort recently is return-oriented programming (ROP) [1711]. Many modern systems try to prevent type safety attacks by data execution prevention – marking memory as either code or data, a measure that goes back to the Burroughs 5000; and if all the code is signed, surely you'd think that unauthorised code cannot be executed? Wrong! An attacker can look for gadgets – sequences of instructions with some useful effect, ending in a return. By collecting enough gadgets, it's possible to assemble a machine that's Turing powerful, and implement our attack code as a chain of ROP gadgets. Then all one has to do is seize control of the call stack. This evolved from the return-to-libc attack which uses the common shared library libc to provide well-understood gadgets; many variants have been developed since, including an attack that enables malware in an SGX enclave to mount stealthy attacks on host apps [1691]. The latest attack variant, block-oriented programming (BOP), can often generate attacks automatically from crashes discovered by program fuzzing, defeating current control-flow integrity controls [966]. This coevolution of attack and defence will no doubt continue.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7461</div><div class='noteText'>Finally there are side channels. The most recent major innovation in attack technology targets CPU pipeline behaviour. In early 2018, two game-changing attacks pioneered the genre: Meltdown, which exploits side-channels created by out-of-order execution on Intel processors [1173], and Spectre, which exploits speculative execution on Intel, AMD and Arm processors [1070]. The basic idea is that large modern CPUs’ pipelines are so long and complex that they look ahead and anticipate the next dozen instructions, even if these are instructions that the current process wouldn't be allowed to execute (imagine the access check is two instructions in the future and the read</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7467</div><div class='noteText'>operation it will forbid is two instructions after that). The path not taken can still load information into a cache and thus leak information in the form of delays. With some cunning, one process can arrange things to read the memory of another.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7488</div><div class='noteText'>Perhaps the most serious example of user interface failure, in terms of the number of systems historically attacked, consists of two facts: first, Windows is forever popping up confirmation dialogues, which trained people to click boxes away to get their work done; and second, that until 2006 a user needed to be the</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7490</div><div class='noteText'>administrator to install anything. The idea was that restricting software installation to admins enabled Microsoft's big corporate customers, such as banks and government departments, to lock down their systems so that staff couldn't run games or other unauthorised software. But in most environments, ordinary people need to install software to get their work done. So hundreds of millions of people had administrator privileges who shouldn't have needed them, and installed malicious code when a website simply popped up a box telling them to do something. This was compounded by the many application developers who insisted that their code run as root, either out of laziness or because they wanted to collect data that they really shouldn't have had. Windows Vista started to move away from this, but a malware ecosystem is now well established in the PC world, and one is starting to take root in the Android ecosystem as businesses pressure people to install apps rather than using websites, and the apps demand access to all sorts of data and services that they really shouldn't have.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7503</div><div class='noteText'>The first consists of specific defences. Stack canaries are a random number inserted by the compiler next to the return address on the stack. If the stack is overwritten, then with high probability the canary will change [484]. Data execution prevention (DEP) marks all memory as either data or code, and prevents the former being executed; it appeared in 2003 with Windows XP. Address space layout randomisation (ASLR) arrived at the same time; by making the memory layout different in each instance of a system, it makes it harder for an attacker to predict target addresses. This is particularly important now that there are toolkits to do ROP attacks, which bypass DEP. Control flow integrity mechanisms involve analysing the possible control-flow graph at compile time and enforcing this at runtime by validating indirect control-flow transfers; this appeared in 2005 and was incorporated in various products over the following decade [351]. However the analysis is not precise, and block-oriented programming attacks are among the tricks that have evolved to exploit the gaps [966].</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7513</div><div class='noteText'>The second consists of better general-purpose tools. Static-analysis programs such as Coverity can find large numbers of potential software bugs and highlight ways in which code deviates from best practice; if used from the start of a project, they can make a big difference.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 6: Access Control &gt; Location 7515</div><div class='noteText'>they can throw up thousands of alerts that are a pain to deal with.) The radical solution is to use a better language; my colleagues increasingly write systems code in Rust rather than in C or C + + 10. The third is better training. In 2002, Microsoft announced a security initiative that involved every programmer being trained in how to write secure code. (The book they produced for this, ‘Writing Secure Code’ [929], is still worth a read.) Other companies followed suit. The latest approach is DevSecOps, which I discuss in section 27.5.6. Agile development methodology is extended to allow very rapid deployment of patches and response to incidents; it may enable the effort put into design, coding and testing to be aimed at the most urgent problems.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 7728</div><div class='noteText'>Locking is one general way to manage contention for resources such as filesystems and to make conflicting updates less likely. Another approach is callback; a server may keep a list of all those clients which rely on it for security state and notify them when the state changes.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 7774</div><div class='noteText'>Another problem is deadlock, where two systems are each waiting for the other to move first. Edsger Dijkstra famously explained this problem, and its possible solutions, via the dining philosophers' problem. A number of philosophers are seated round a table, with a chopstick between each of them; and a philosopher can only eat when they can pick up the two chopsticks on either side. So if all of them try to eat at once and each picks up the chopstick on their right, they get stuck</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 7792</div><div class='noteText'>When designing protocols that update the state of a distributed system, the ‘motherhood and apple pie’ is ACID – that transactions should be atomic, consistent,</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 7793</div><div class='noteText'>isolated and durable. A transaction is atomic if you ‘do it all or not at all’ – which makes it easier to recover after a failure. It is consistent if some invariant is preserved, such as that the books must still balance.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 7817</div><div class='noteText'>The final concurrency problem of special interest to the security engineer is the provision of accurate time. As authentication protocols such as Kerberos can be attacked by inducing clock error, it's not enough to simply trust a random external time source. One possibility is a Cinderella attack: if a security critical program such as a firewall has a licence with a</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 7820</div><div class='noteText'>timelock, an attacker might wind your clock forward “and cause your firewall to turn into a pumpkin”.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 7851</div><div class='noteText'>a fault may cause an error, which is an incorrect state; this may lead to a failure, which is a deviation from the system's specified behavior. The resilience which we build into a system to tolerate faults and recover from failures will have a number of components, such as fault detection, error recovery and if necessary failure recovery. The meaning of mean-time-before-failure (MTBF)</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 7860</div><div class='noteText'>First, the failures with which we are concerned may be normal or malicious, and we often model the latter as Byzantine. Byzantine failures are inspired by the idea that there are generals defending Byzantium, of whom have been bribed by the attacking Turks to cause as much confusion as possible. The generals can pass oral messages by courier, and the couriers are trustworthy, so each general can exchange confidential and authentic communications with each other general (we could imagine them encrypting and computing a MAC on each message). What is the maximum number</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 7865</div><div class='noteText'>of traitors that can be tolerated? The key observation is that if we have only three generals, say Anthony, Basil and Charalampos, and Anthony is the traitor, then he can tell Basil “let's attack” and Charalampos “let's retreat”. Basil can now say to Charalampos “Anthony says let's attack”, but this doesn't let Charalampos conclude that Anthony's the traitor. It could just as easily have been Basil; Anthony could have said “let's retreat” to both of them, but Basil lied when he said “Anthony says let's attack”. This beautiful insight is due to Leslie Lamport, Robert Shostak and Marshall Pease, who proved that the problem has a solution if and only if [1126]. Of course, if the generals are able to sign their messages, then no general dare say different things to two different colleagues. This illustrates the power of digital signatures in particular and of end-to-end security mechanisms in general. There is now a substantial literature on Byzantine fault tolerance – the detailed design of systems able to withstand this kind of failure; see for example the algorithm by Miguel Castro and Barbara Liskov [396]. Another lesson is that if a component which fails (or can be induced to fail by an opponent) gives the wrong answer rather than just no answer, then it's much harder to use it to build a resilient system. It can be useful if components that fail just stop, or if they can at least be quickly identified and blacklisted.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 7881</div><div class='noteText'>The two most obvious are by using redundancy and fail-stop processes. The latter process error-correction information along with data, and stop when an inconsistency is detected;</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 7951</div><div class='noteText'>there is process group redundancy. Here, we may run multiple copies of a system on multiple servers in different locations and compare their outputs. This can stop the kind of attack in which the opponent gets physical access to a machine and subverts it, whether by mechanical destruction or by inserting unauthorised software. It can't defend against attacks by authorised users or damage by bad authorised software, which could simply order the deletion of a critical file.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 7955</div><div class='noteText'>The next level is backup, where we typically take a copy of the system (a checkpoint) at regular intervals. The copies are usually kept on media that can't be overwritten such as write-protected tapes or discs with special software. We may also keep journals of all the transactions applied between checkpoints. Whatever the detail, backup and recovery mechanisms not only enable us to recover from physical asset destruction, they also ensure that if we do get an attack at the logical level, we have some hope of recovering. The classic example in the 1980s would have been a time bomb that deletes the customer database on a specific date; since the arrival of cryptocurrency, the fashion has been for ransomware. Businesses with critical service requirements, such as banks and retailers, have had backup data centres for many years. The idea is that if the main centre goes down, the service will failover to a second facility. Maintaining such facilities absorbed most of a typical bank's information security budget. Backup is not the same as fallback. A fallback system is typically a less capable system to which processing reverts when the main system is unavailable.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 8050</div><div class='noteText'>The basic algorithm used to bind names to addresses is known as rendezvous: the principal exporting a name advertises it somewhere, and the principal seeking to import and use it searches for it. Obvious examples include phone books and file system directories.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 8055</div><div class='noteText'>The function of names is to facilitate sharing.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 8060</div><div class='noteText'>The naming information may not all be in one place, and so resolving names brings all the general</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 8061</div><div class='noteText'>problems of a distributed system.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 8068</div><div class='noteText'>It is bad to assume that only so many names will be needed. The</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 8073</div><div class='noteText'>Global names buy you less than you think.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 8079</div><div class='noteText'>Names imply commitments, so keep the scheme flexible enough to cope with organisational changes.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 8084</div><div class='noteText'>Names may double as access tickets, or capabilities.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 8095</div><div class='noteText'>Things are made much simpler if an incorrect name is obvious.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 8102</div><div class='noteText'>Consistency is hard, and is often fudged. If directories are replicated, then you may find yourself unable to read, or to write, depending on whether too many or too few directories are available.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 7: Distributed Systems &gt; Location 8110</div><div class='noteText'>Don't get too smart. Phone numbers are much more robust than computer addresses.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 8: Economics &gt; Location 8664</div><div class='noteText'>The main tool we can use to study and analyse them is game theory – the study of problems of cooperation and conflict among independent decision makers. Game theory provides a common language used by economists, biologists and political scientists as well as computer scientists, and is a useful tool for building collaboration across disciplines.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 8: Economics &gt; Location 8668</div><div class='noteText'>Alice and Bob toss coins and reveal them simultaneously, upon which Alice gets Bob's penny if they're different and Bob gets Alice's penny if they're the same. I'll write this as in Figure 8.2: Bob H T Alice H − 1,1 1, − 1 T 1, − 1 − 1,1 Figure 8.2: Matching pennies Each entry in the table shows first Alice's outcome and then Bob's. Thus if the coins fall (H, H) Alice loses a penny and Bob gains a penny. This is an example of a zero-sum game: Alice's gain is Bob's loss.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 8: Economics &gt; Location 8681</div><div class='noteText'>Often we can solve a game quickly by writing out a payoff matrix like this. Here's an example (Figure 8.3): Bob Left Right Alice Top 1,2 0,1 Bottom 2,1 1,0 Figure 8.3: Dominant strategy equilibrium</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 8: Economics &gt; Location 8692</div><div class='noteText'>In game theory, a strategy is just an algorithm that takes a game state and outputs a move1. In this game, no matter what Bob plays, Alice is better off playing ‘Bottom’; and no matter what Alice plays, Bob is better off playing ‘Left’. Each player has a dominant strategy – an optimal choice regardless of what the other does. So Alice's strategy should be a constant ‘Bottom’ and Bob's a constant ‘Left’. We call this a dominant strategy equilibrium. Another example is shown in Figure 8.4: Bob</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 8: Economics &gt; Location 8701</div><div class='noteText'>Left Right Alice Top 2,1 0,0 Bottom 0,0 1,2</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 8: Economics &gt; Location 8712</div><div class='noteText'>We're now ready to look at a famous problem that applies to many situations from international trade negotiations through cooperation between hunting animals to whether the autonomous systems that make up the Internet cooperate effectively to protect its infrastructure. It was first studied by scientists at the Rand corporation in 1950 in the context of US and USSR defense spending; Rand was paid to think about possible strategies in nuclear war. But they presented it using the following simple example. Two prisoners are arrested on suspicion of planning a bank robbery. The police interview them separately and tell each of them: “If neither of you confesses you'll each get a year for carrying a concealed firearm without a permit. If only one of you confesses, he'll go free and the other will get 6 years for conspiracy to rob. If both of you confess, you will each get three years.” What should the prisoners do? Figure 8.5 shows their payoff matrix: Benjy Confess Deny Alfie Confess − 3,-3 0,-6 Deny − 6,0 − 1, − 1</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 8: Economics &gt; Location 8730</div><div class='noteText'>When Alfie looks at this table, he will reason as follows: “If Benjy's going to confess then I should too as then I get 3 years rather than 6; and if he's going to deny then I should still confess as I'll walk rather than doing a year”. Benjy will reason similarly. The two of them confess, and get three years each. This is not just a Nash equilibrium; it's a dominant strategy equilibrium. Each prisoner should confess regardless of what the other does. But hang on, you say, if they had agreed to keep quiet then they'll get a year each, which is a better outcome for them! In fact the strategy (deny, deny) is Pareto efficient, while the dominant strategy equilibrium is not. (That's one reason it's useful to have concepts like ‘Pareto efficient’ and ‘dominant strategy equilibrium’ rather than just arguing over ‘best’.)</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 8: Economics &gt; Location 8737</div><div class='noteText'>going to be played once only, and this is the only game in town, there isn't a solution. Both prisoners will confess and get three years.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 8: Economics &gt; Location 8814</div><div class='noteText'>In the English, or ascending-bid, auction, the auctioneer starts at a reserve price and then raises the price until only one bidder is left. This is used to sell art and antiques. In the Dutch, or descending-bid, auction, the auctioneer starts out at a high price and cuts it gradually until someone bids. This is used to sell flowers. In the first-price sealed-bid auction, each bidder is allowed to make one bid. After bidding closes, all the bids are opened and the highest bid wins. This has been used to auction TV rights; it's also used for government contracts, where it's the lowest bid</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 8: Economics &gt; Location 8819</div><div class='noteText'>that wins. In the second-price sealed-bid auction, or Vickrey auction, we also get sealed bids and the highest bid wins, but that bidder pays the price in the second-highest bid. This is familiar from eBay, and is also how online ad auctions work; it evolved to sell rare postage stamps, though the earliest known use was by the poet Goethe to sell a manuscript to a publisher in the 18th century. In the all-pay auction, every bidder pays at every round, until all but one drop out. This is a model of war, litigation, or a winner-take-all market race between several tech startups. It's also used for charity fundraising.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 8: Economics &gt; Location 8825</div><div class='noteText'>The first key concept is strategic equivalence. The Dutch auction and the first-price sealed-bid auction give the same result, in that the highest bidder gets the goods at his reservation price – the maximum he's prepared to bid. Similarly, the English auction and the Vickrey auction give the same result (modulo the bid increment). However the two pairs are not strategically equivalent. In a Dutch auction, you should bid low if you believe your valuation is a lot higher than anybody else's, while in a second-price auction it's best to bid truthfully. The second key concept is revenue equivalence. This is a weaker concept; it's not about who will win, but how much money the auction is expected to raise. The interesting result here is the revenue equivalence theorem, which says that you get the same revenue from any well-behaved auction under ideal conditions. These conditions include risk-neutral bidders, no collusion, Pareto efficiency (the highest bidder gets the goods) and independent valuations (no externalities between bidders). In</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 8: Economics &gt; Location 8902</div><div class='noteText'>By then, we understood the basics of information economics: the combination of high fixed and low marginal costs, network effects and technical lock-in makes platform markets particularly likely to be dominated by single vendors, who stand to gain vast fortunes if they can win the race to dominate the market. In such a race, the Microsoft philosophy of the 1990s – ‘ship it Tuesday and get it right by version 3’ – is perfectly rational behaviour. In such a race, the platform vendor must appeal not just to users but also to complementers – to the software companies who decide whether to write applications for its platform or for someone else's. Security gets in the way of applications, and it tends to be a lemons market anyway. So the rational vendor engaged in a race for platform dominance will enable all applications to run as root on his platform2, until his position is secure. Then he may add more security – but will be tempted to engineer it in such a way as to maximise customer lock-in, or to appeal to complementers in new markets such as digital media. The same pattern was also seen in other platform products, from the old IBM mainframe operating systems through telephone exchange switches to the early Symbian operating system for mobile phones. Products are insecure at first, and although they improve over time, many of the new security features are for the vendor's benefit as much as the user's. And this is exactly what we saw with Microsoft's product lines. DOS had no protection at all and kick-started the malware market; Windows 3 and Windows 95 were dreadful; Windows 98 was only slightly better; and security problems eventually so annoyed Microsoft's customers that finally in 2003 Bill Gates decided to halt development until all its engineers had been on a secure coding course. This was followed by investment in better testing, static analysis tools, and regular patching. The number and lifetime of exploitable vulnerabilities continued to fall through later releases of Windows. But the attackers got better too, and the protection in Windows isn't all for the user's benefit. As</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 8: Economics &gt; Location 8918</div><div class='noteText'>Peter Gutmann points out, much more effort went into protecting premium video content than into protecting users’ credit card numbers [843].</h3>
<h2 class='sectionHeading'>PART II</h2><h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9417</div><div class='noteText'>A security policy is a succinct description of what we're trying to achieve; it's driven by an understanding of the bad outcomes we wish to avoid and in turn drives the engineering.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9436</div><div class='noteText'>Where a top-down approach to security engineering is possible, it will typically take the form of threat model – security policy – security mechanisms. The critical, and often neglected, part of this process is the security policy. By a security policy, we mean a document that expresses clearly and concisely what the protection mechanisms are to achieve. It is driven by our understanding of threats, and in turn drives our system design. It will often take the form of statements about which users may access which data. It plays the same role in specifying the system's protection requirements, and evaluating whether they have been met, that the system specification does for functionality and the safety case for safety.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9461</div><div class='noteText'>A security policy model is a succinct statement of the protection properties that a system must have. Its key points can typically be written down in a page or less. It is the document in which the protection goals of the system are agreed with an entire community, or with the top management of a customer. It may also be the basis of formal mathematical analysis. A security target is a more detailed description of the protection mechanisms that a specific implementation provides, and how they relate to a list of control objectives (some but not all of which are typically derived from the policy model). The security target forms the basis for testing and evaluation of a product. A protection profile is like a security target but expressed in a manner that is independent of the implementation, so as to enable comparable evaluations across products and versions. This can involve the use of a semi-formal language, or at least of suitable security jargon. A protection profile is a requirement for products that are to be evaluated under the Common Criteria [1398]. (I discuss the Common Criteria in section 28.2.7; they are used by many governments for mutual recognition of security evaluations of defense information systems.)</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9510</div><div class='noteText'>A classification label, plus a set of codewords, makes up a security category or (if there's at least one codeword) a compartment, which is a set of records with the same access control policy. Compartmentation is typically implemented nowadays using discretionary access control mechanisms;</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9513</div><div class='noteText'>There are also descriptors, caveats and IDO markings. Descriptors are words such as ‘Management’, ‘Budget’, and ‘Appointments’: they do not invoke any special handling requirements, so we can deal with a file marked ‘Confidential – Management’ as if it were simply marked ‘Confidential’. Caveats are warnings such as “UK Eyes Only”, or the US equivalent, “NOFORN”; they do create restrictions. There are also International Defence Organisation markings such as NATO5.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9536</div><div class='noteText'>reference monitor – a component of the operating system that would mediate access control decisions and be small enough to be subject to analysis and tests, the completeness of which could be assured. In modern parlance, such components – together with their associated operating procedures – make up the Trusted Computing Base (TCB). More formally, the TCB is defined as the set of components (hardware, software, human, …) whose correct functioning is sufficient to ensure that the security policy is enforced, or, more vividly, whose failure could cause a breach of the security policy.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9545</div><div class='noteText'>Bell-LaPadula (BLP) model enforces two properties: The simple security property: no process may read data at a higher level. This is also known as no read up (NRU);</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9547</div><div class='noteText'>The *-property: no process may write data to a lower level. This is also known as no write down (NWD).</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9570</div><div class='noteText'>The first big controversy was about John McLean's System Z, which he defined as a BLP system with the added feature that a user can ask the system administrator to temporarily declassify any file from High to Low.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9574</div><div class='noteText'>The issue is dealt with by introducing a tranquility property. Strong tranquility says that security labels never change during system operation, while weak tranquility says that labels never change in such a way as to violate a defined security policy.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9592</div><div class='noteText'>Another problem with BLP, and indeed with all mandatory access control systems, is that separating users and processes is the easy part; the hard part is when some controlled interaction is needed.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9613</div><div class='noteText'>The torrent of research money that poured into multilevel security from the 1980s led to a number of alternative formulations. Noninterference was introduced by Joseph Goguen and Jose Meseguer in 1982 [774]. In a system with this property, High's actions have no effect on what Low can see. Nondeducibility is less restrictive and was introduced by David Sutherland in 1986 [1851] to model applications such as a LAN on which there are machines at both Low and High, with the High machines encrypting their LAN traffic6. Nondeducibility turned out to be too weak, as there's nothing to stop Low making deductions about High input with 99% certainty. Other theoretical models include Generalized Noninterference and restrictiveness [1278]; the Harrison-Ruzzo-Ullman model tackles the problem of how to deal with the creation and deletion of files, on which BLP is silent [869]; and the Compartmented Mode Workstation (CMW) policy attempted to model the classification of information using floating labels, as in the high water mark policy [808,2042].</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9626</div><div class='noteText'>type enforcement (TE) model, due to Earl Boebert and</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9627</div><div class='noteText'>Dick Kain [272], later extended by Lee Badger and others to Domain and Type Enforcement (DTE) [154]. This assigns subjects to domains and objects to types, with matrices defining permitted domain-domain and domain-type interactions.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9782</div><div class='noteText'>The case of SELinux is somewhat similar to Windows in that the immediate goal of mandatory access control mechanisms was also to limit the effects of a compromise. SELinux [1189] was implemented by the NSA, based on the Flask security architecture [1815], which separates the policy from the enforcement mechanism; a security context contains all of the security attributes associated with a subject or object in Flask, where one of those attributes includes the Type Enforcement type attribute. A security identifier is a handle to a security context, mapped by the security server. This is where policy decisions are made and resides in the kernel for performance [820]. It has been mainstream since Linux 2.6. The server provides a security API to the rest of the kernel, behind which the security model is hidden. The server internally implements a general constraints engine that can express RBAC, TE, and MLS. In typical Linux distributions from the mid-2000s, it was used to separate various services, so an attacker who takes over your web server does not thereby acquire your DNS server as well. Its adoption by Android has made it part of the world's most popular operating system, as described in Chapter 6.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9811</div><div class='noteText'>9.6.1 Composability Consider a simple device that accepts two ‘High’ inputs and ; multiplexes them; encrypts them by xor'ing them with a one-time pad (i.e., a random generator); outputs the other copy of the pad on ; and outputs the ciphertext, which being encrypted with a cipher system giving perfect secrecy, is considered to be low (output ), as in Figure 9.4. In isolation, this device is provably secure. However, if feedback is permitted, then the output from can be fed back into , with the result that the high input now appears at the low output . Timing inconsistencies can also break the composition of two secure systems (noted by Daryl McCullough [1262]).</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9824</div><div class='noteText'>In general, the composition problem – how to compose two or more secure components into a secure system – is hard, even at the relatively uncluttered level of proving results about ideal components [1432]. (Simple information flow doesn't compose; neither does noninterference or nondeducibility.) Most of the low-level problems arise when some sort of feedback is introduced; without it, composition can be achieved under a number of formal models [1279]. However, in real life, feedback is pervasive, and composition of security properties can be made even harder by interface issues, feature interactions and so on.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9833</div><div class='noteText'>The cascade problem An example of the composition problem is given by the cascade problem (Figure 9.5). After the Orange book introduced a series of evaluation levels, this led to span-limit rules about the number of levels at which a system can operate [548]. For example, a system evaluated to B3 was in general allowed to process information at Unclassified, Confidential and Secret, or at Confidential, Secret and Top Secret; there was no system permitted to process Unclassified and Top Secret data simultaneously [548]. As the diagram shows, it is straightforward to connect together two B3 systems in such a way that this policy is broken. The first system connects together Unclassified and Secret, and its Secret level communicates with the second system – which also processes Top Secret information [925]. This defeats the span limit.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9849</div><div class='noteText'>a covert channel is a mechanism that was not designed for communication but that can nonetheless be abused to allow information to be communicated down from High to Low.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9868</div><div class='noteText'>The defense computer community was shocked when Fred Cohen wrote the first thesis on computer viruses, and used a virus to penetrate multilevel secure systems easily in 1983. In his first experiment, a file virus that took only eight hours to write managed to penetrate a system previously believed to be multilevel secure [452]. People had been thinking about malware since the 1960s and had done various things to mitigate it, but their focus had been on Trojans. There are many ways in which malicious code can be used to break access controls. If the reference monitor (or other TCB components) can be corrupted, then malware can deliver the entire system to the attacker, for example by issuing an unauthorised clearance. For this reason, slightly looser rules apply to so-called closed security environments which are defined to be those where ‘system applications are adequately protected against the insertion of malicious logic’ [548], and this in turn created an incentive for vendors to tamper-proof the TCB, using techniques such as TPMs. But even if the TCB remains intact, malware could still copy itself up from Low to High (which BLP doesn't prevent) and use a covert channel to signal information down.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9880</div><div class='noteText'>Another problem that exercised the research community is polyinstantiation. Suppose our High user has created a file named agents, and our Low user now tries to do the same. If the MLS operating system prohibits him, it will have leaked information – namely</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9882</div><div class='noteText'>that there is a file called agents at High. But if it lets him, it will now have two files with the same name.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9913</div><div class='noteText'>Multilevel secure systems are surprisingly expensive and difficult to build and deploy. There are many sources of cost and confusion. They are built in small volumes, and often to high standards of physical robustness, using elaborate documentation, testing and other quality control measures driven by military purchasing bureaucracies. MLS systems have idiosyncratic administration tools and procedures. A trained Unix administrator can't just take on an MLS installation without significant further training; so many MLS systems are installed without their features being used. Many applications need to be rewritten or at least greatly modified to run under MLS operating</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9920</div><div class='noteText'>Because processes are automatically upgraded as they see new labels, the files they use have to be too. New files default to the highest label belonging to any possible input. The result of all this is a chronic tendency for things to be overclassified. There's a particular problem when system components accumulate all the labels they've seen, leading to label explosion where they acquire such a collection that no single principal can access them any more. So they get put in the trusted computing base, which ends up containing a quite uncomfortably large part of the operating system (plus utilities, plus windowing system software, plus middleware such as database software). This ‘TCB bloat’ constantly pushes up the cost of evaluation and reduces assurance.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 9: Multilevel Security &gt; Location 9939</div><div class='noteText'>Although MLS systems can prevent undesired things (such as information leakage), they also prevent desired things too (such as building a search engine to operate across all an agency's Top Secret compartmented data). So even in military environments, the benefits can be questionable.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 11: Inference Control &gt; Location 10866</div><div class='noteText'>Anonymisation is an aspirational term that means stripping identifying information from data in such a way that useful statistical research can be done without leaking information about identifiable data subjects.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 11: Inference Control &gt; Location 10924</div><div class='noteText'>Combining low-level sources to draw a high-level conclusion is known as an aggregation attack in the national security context.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 11: Inference Control &gt; Location 10931</div><div class='noteText'>A characteristic formula is the expression (in some database query language) that selects a query set of records. An example might be ‘all female employees of the Computer Laboratory at the grade of professor’. The smallest query sets, obtained by the logical AND of all the attributes (or their negations) are known as elementary sets or cells. The statistics corresponding to query sets may be sensitive statistics if the set size is too small. The objective of inference control is to prevent the disclosure of sensitive statistics. If we let be the set of statistics that are disclosed and the set that are sensitive and must be protected, then we need for privacy, where is the complement of . If , then the protection is said to be precise. Protection that is not precise will usually carry some cost in terms of the range of queries that the database can answer and may therefore degrade its usefulness.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 11: Inference Control &gt; Location 10943</div><div class='noteText'>The simplest protection mechanism is to specify a minimum query set size, so that no question is answered if the number of records from which the answer is calculated is less than some threshold . But this is not enough. Say = 6; then an obvious tracker attack is to make an enquiry on six patients' records, and then on those records plus the target's. And you must also prevent the attacker from querying all but one of the records: if there are records and a query set size threshold of , then between and records must be the subject of a query for it to be allowed. This also applies to subsets. For example, when I wrote the first edition of this book, only one of the full professors in our lab was female. So we could have found out her salary with just two queries: ‘Average salary professors?’ and ‘Average salary male professors?’. So you have to avoid successive queries of record sets and if and .</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 11: Inference Control &gt; Location 10954</div><div class='noteText'>11.2.1.2 Trackers That is an example of an individual tracker, a custom formula that allows us to calculate the answer to a forbidden query indirectly. There are also general trackers – sets of formulae that will enable any sensitive statistic to be revealed. A somewhat depressing discovery made in the late 1970s, due to Dorothy Denning, Peter Denning and Mayer Schwartz, was that general trackers are usually easy to find. Provided the minimum query set size is less than a quarter of the total number of statistics , and there are no further restrictions on the type of queries that are allowed, then we can find formulae that provide general trackers [541]. So tracker attacks are easy, unless we restrict the query set size or control the allowed queries in some other way. Such query auditing turns out to be an NP-complete problem.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 11: Inference Control &gt; Location 11019</div><div class='noteText'>Another approach is k-anonymity, due to Pierangela Samarati and Latanya Sweeney, which means that each individual whose data is used in calculating a release of data cannot be distinguished from others [1646]. Its limitation is that it's an operational definition of a privacy mechanism rather than a mathematical definition of a privacy property; it's not much help if individuals all possess the same sensitive attribute. Where the database is open for online queries, we can use implied queries control: we allow a query on attribute values only if every one of the implied query sets given by setting the attributes to true or false, has at least records. An alternative is to limit the type of inquiries. Maximum order control limits the number of attributes any query can have. However, to be effective, the limit may have to be severe. It takes only 33 bits of information to identify a human, and most datasets are of much smaller populations. A more thorough approach (where it is feasible) is to reject queries that would partition the sample population into too many sets.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 11: Inference Control &gt; Location 11051</div><div class='noteText'>Some systems try to get round the limits imposed by static query control by keeping track of who accessed what. Known as query overlap control, this involves rejecting any query from a user that, combined with what the user knows already, would disclose a sensitive statistic. This may sound like a good idea, but in practice it suffers from two usually insurmountable drawbacks. First, the complexity of the processing involved increases over time, and often exponentially. Second, it's extremely hard to be sure that your users</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 11: Inference Control &gt; Location 11055</div><div class='noteText'>don't collude, or that one user has registered under two different names. Even if your users are all honest and distinct persons today, it's always possible that one of them will get taken over tomorrow.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 11: Inference Control &gt; Location 11058</div><div class='noteText'>By now it should be clear that if various kinds of query control are the only protection mechanisms used in a statistical database, they will often impose an unacceptable statistical performance penalty. So query control is often used in conjunction with various kinds of randomization, designed to degrade the signal-to-noise ratio from the attacker's point of view while impairing that of the legitimate user as little as possible.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 11: Inference Control &gt; Location 11061</div><div class='noteText'>Until 2006, all the methods used were rather ad hoc. They started with perturbation, or adding noise with zero mean and a known variance to the data; but this tends to damage the legitimate user's results precisely when the sample set sizes are small, and leave them intact when the sample sets are large enough to use simple query controls anyway. A later variant was controlled tabular adjustment where you identify the sensitive cells and replace their values with different ones, then adjust other values in the table to restore additive relationships [490]. Then there are random sample queries where we make all the query sets the same size, selecting them at random from the available relevant statistics. Thus, all the released data are computed from small samples rather than from the whole database, and we can use a pseudorandom number generator keyed to the input query to make the results repeatable. Random sample queries are a natural protection mechanism where the correlations being investigated are strong enough that a small sample is sufficient. Finally, there's swapping, another of Tore Dalenius' innovations; many census bureaux swap a proportion of records so that a family with two young teenage kids and an income in the second quartile might be swapped for a similar family in a town in the next county.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 11: Inference Control &gt; Location 11085</div><div class='noteText'>we have a solid theory of exactly how much protection we can get from adding randomness: differential privacy. This is now being used for the 2020 US census, and we'll discuss it in more detail later in this chapter.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 11: Inference Control &gt; Location 11311</div><div class='noteText'>differential privacy, enables the security engineer to limit the probability of disclosure, even in the presence of an adversary with unbounded computational power and copious side information, and can thus be seen as the equivalent of the one-time pad and unconditionally secure authentication codes in cryptography. Although it started as a paper on theoretical cryptography, it has come to be seen as the gold standard for both statistical database security and for anonymisation in general. The starting point was an earlier paper by Kobbi Nissim and Irit Dinur, who</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 11: Inference Control &gt; Location 11315</div><div class='noteText'>had shown in 2003 that if queries on a database each returned an approximation to a linear function of private bits of information, then so long as the error was small enough the number of queries required to reconstruct the database would not grow too quickly; such reconstruction attacks are, after all, based on linear algebra, so rather than making carefully targeted tracker attacks, an attacker can just make a whole lot of random queries, then do the algebra and get everything out [562]. So the defender has to add noise if there will be more than a limited number of queries, and the question is how much.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 11: Inference Control &gt; Location 11320</div><div class='noteText'>The key insight of differential privacy is that, to avoid inadvertent disclosure, no individual's contribution to the results of queries should make too much of a difference, so you calibrate the standard deviation of the noise according to the sensitivity of the data. A privacy mechanism is called -indistinguishable if for all databases and differing in a single row, the probability of getting any answer from is within a multiplicative factor of of getting it from ; in other words, you bound the logarithm of the ratios. It follows that you can use noise with a Laplace distribution to get indistinguishability with noisy sums, and things compose, so it all becomes mathematically tractable. The value of , which sets the trade-off between accuracy and privacy, has to be set by policy. Small values give strong privacy; but setting = 1000 is basically publishing your raw data.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 13: Locks and Alarms &gt; Location 13272</div><div class='noteText'>The design and testing of entry controls and alarms are driven by a policy based on: Deter – detect – alarm – delay – respond A facility can deter intruders using hard methods such as concrete walls, or softer methods such as being inconspicuous. It will then have one or more layers of barriers and sensors whose job is to keep out casual intruders, detect deliberate intruders, and make it difficult for them to get in too quickly. This will be complemented by an alarm system designed to get a response to the scene in time. As the barriers will have doors for authorized staff to go in and out, there will be entry control that could be anything from metal keys to biometric scanners.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 13: Locks and Alarms &gt; Location 13406</div><div class='noteText'>The first of these is bumping. This technique enables many mechanical locks to be opened quickly and without damage by unskilled people using tools that are now readily available. Its main target is the pin-tumbler lock originally patented by Linus Yale in 1860</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 13: Locks and Alarms &gt; Location 13410</div><div class='noteText'>These locks have a cylindrical plug set inside a shell, and prevented from rotating by a number of pin stacks. Each stack usually consists of two or three pins, one on top of the other. The bottom pin or key pin makes direct contact with the key; behind it is a spring-loaded top pin or driver pin that forces the bottom pin as far down as possible in the keyway. When the correct key is inserted, the gaps between the top pin and the bottom pin in each stack align with the edge of the plug, creating a shear line; the plug can now be turned. A typical house or office lock might have five or six pins each of which could have the gap in ten different positions, giving a theoretical key diversity of or possible key differs. The actual number will be less because of mechanical tolerances and key-cutting restrictions.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 13: Locks and Alarms &gt; Location 13469</div><div class='noteText'>The second recent class of problems are master-key attacks. These were also known to locksmiths for some time but were improved and published by Matt Blaze1. Master-key systems are designed so that in addition to the individual key for each door in a building, there can be a top-level master key that opens them all – say, for use by the cleaners. More complex schemes are common; in our building, for example, I can open my students' doors while the system administrators and cleaners can open mine. In pin-tumbler locks, such schemes are implemented by having extra cuts in some of the pin stacks. Thus instead of having a top pin and a bottom pin with a single cut between them, some of the pin stacks will have a middle pin as well.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 13: Locks and Alarms &gt; Location 13616</div><div class='noteText'>Burglar alarms use a wide range of sensors, including: vibration detectors, to sense fence disturbance, footsteps, breaking glass or other attacks on buildings or perimeters; switches on doors and windows; passive infrared devices to detect body heat; motion detectors using ultrasonics or microwave; invisible barriers of microwave or infrared beams; pressure pads under the carpet, which in extreme cases may extend to instrumenting the entire floor with pressure transducers under each tile; video cameras, nowadays often with movement detectors and even face detectors, to alarm automatically or provide a live video feed to a monitoring center; movement sensors on equipment, ranging from simple tie-down cables through seismometers to loops of optical fiber.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 16: Security Printing and Seals &gt; Location 15107</div><div class='noteText'>The industry distinguishes three different levels of inspection of a forged banknote or document [1939]: a primary inspection is one performed by an untrained inexperienced person, such as a member of the public or a new cashier at a store. Often the primary inspector has no motivation, or even a negative motivation. If he gets a banknote that feels slightly dodgy, he may try to pass it on without looking at it closely enough to have to decide between becoming an accomplice or going to the hassle of reporting it; a secondary inspection is one performed in the field by a competent and motivated person, such as an experienced bank teller in the case of banknotes or a trained manufacturer's inspector in the case of product labels. This person may have some special equipment such as an ultraviolet lamp, a pen with a chemical reagent, or even a scanner and a PC. However the equipment will be limited in both cost and bulk, and will be completely understood by serious counterfeiters; a tertiary inspection is one performed at the laboratory of the manufacturer or the note issuing bank. The experts who designed the security printing (and perhaps even the underlying industrial processes) will be on hand, with substantial equipment and support.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 16: Security Printing and Seals &gt; Location 15126</div><div class='noteText'>intaglio, a process where an engraved pattern is used to press the ink on to the paper with great force, leaving a raised ink impression with high definition. This is often used for scroll work on paper banknotes and passports;</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 16: Security Printing and Seals &gt; Location 15128</div><div class='noteText'>letterpress in which the ink is rolled on raised type that is then pressed on to the page, leaving a depression. The numbers on paper banknotes are usually printed this way, often with numbers of different sizes and using different inks to prevent off-the-shelf numbering equipment being used;</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 16: Security Printing and Seals &gt; Location 15130</div><div class='noteText'>special printing presses, called Simultan presses, which transfer all the inks, for both front and back, to the paper simultaneously. The printing on front and back can therefore be accurately aligned; patterns can be printed partly on the front and partly on the back so that they match up perfectly when the note is held up to the light (see-through register). Reproducing this is believed to be hard on cheap color printing equipment. Simultan presses also have special ducting to make ink colors vary along the line (rainbowing);</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 16: Security Printing and Seals &gt; Location 15135</div><div class='noteText'>rubber stamps that are used to endorse documents, or to seal photographs to them; embossing and laminates that are also used to seal photographs, and on bank cards to push up the cost of forgery. Embossing can be physical, or use laser engraving techniques to burn a photo into an ID card; watermarks are an example of putting protection features in the paper. They are more translucent areas inserted into the paper by varying its thickness when it is manufactured. Many other special materials, such as fluorescent threads, are used for similar purposes.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 16: Security Printing and Seals &gt; Location 15184</div><div class='noteText'>The lessons drawn so far are [1939]: security features should convey a message relevant to the product. So it's better to use iridescent ink to print the denomination of a banknote than some obscure feature nobody looks at; security features should obviously belong where they are, so they become embedded in the user's cognitive model of the object; their effects should be obvious, distinct and intelligible; they should not have existing competitors that can provide a basis for imitations; they should be standardized.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 17: Biometrics &gt; Location 15614</div><div class='noteText'>Like alarms, most biometric systems have a tradeoff between false-accept and false-reject rates, often referred to in the banking industry as the fraud and insult rates and in the biometric literature as type 1 and type 2 errors. Many systems can be tuned to favor one over the other. The tradeoff is known as the receiver operating characteristic, a term first used by radar operators; if you turn up the gain on your radar set too high, you can't see the target for clutter, while if it's too low, you can't see it at all. So the radar operator has to select a suitable point on the curve. The equal error rate is when the system is tuned so that the probabilities of false-accept and</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 17: Biometrics &gt; Location 15794</div><div class='noteText'>These are examples of one type of identity verification – checking against a blacklist. The other type is where the system checks a claim to identity, with the main US applications being building entry control and welfare payment</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 17: Biometrics &gt; Location 15978</div><div class='noteText'>One practical problem with iris scanning used to be getting the picture cheaply without being too intrusive.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 17: Biometrics &gt; Location 15994</div><div class='noteText'>Possible attacks on iris recognition systems include – in unattended operation at least – a simple photograph of the target's iris. There are terminals that will detect such simple fakes, for example by measuring hippus – a natural fluctuation in the diameter of the pupil that happens at about 0.5   Hz. But the widely-sold cheap terminals don't do this, and if liveness detection became widespread then no doubt attackers would try more sophisticated tricks, such as printing the target's iris patterns on a contact lens.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 17: Biometrics &gt; Location 16009</div><div class='noteText'>Voice recognition – also known as speaker recognition – is the problem of identifying a speaker from a short utterance. While speech recognition systems are concerned with transcribing speech and need to ignore speech idiosyncrasies, voice recognition systems need to amplify and classify them.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 17: Biometrics &gt; Location 16025</div><div class='noteText'>there is a description of a 1990s system fielded in US EP-3 aircraft that breaks up intercepted messages from enemy aircraft and ground controllers into quarter second segments that are then cut and pasted to provide new, deceptive messages.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 17: Biometrics &gt; Location 16079</div><div class='noteText'>Many interesting attacks are more specific to biometric systems and apply to more than one type of biometric. Forensic biometrics often don't tell as much as one might assume. Apart from the possibility that a fingerprint or DNA sample might have been planted by the police, it may just be old. The age of a fingerprint can't be determined directly, and prints on areas with public access say little. A print on a bank door says much less than a print in a robbed vault. So in premises vulnerable to robbery, cleaning procedures may be critical for evidence. If a suspect's prints are found on a bank counter, and he claims that he had gone there three days previously, he may be convicted by evidence that the branch counter is polished every evening. Putting this in system terms, freshness is often a critical issue, and some quite unexpected things can find themselves inside the ‘trusted computing base’. Another aspect of freshness is that most biometric systems can, at least in theory, be attacked using suitable recordings. We mentioned direct attacks on voice recognition, attacks on iris scanners by photos on a contact lens, and moulds of fingerprints. Even simpler still, in countries like South Africa where fingerprints are used to pay pensions, there are persistent tales of ‘Granny's finger in the pickle jar’ being the most valuable property she bequeathed to her family. The lesson to be learned here is that unattended operation of biometric authentication devices is tricky. Attacks aren't always straightforward; although it's easy to make a mould from a good fingerprint [408], the casual prints that people leave lying around on doorknobs, beer glasses and so on are often too smudged and fragmentary to pass an identification system. But attacks are definitely possible, and definitely happen. Defences are also possible; voice recognition systems can demand that you read out an unpredictable challenge to thwart recordings, while one version of the app that EU citizens use to apply for residence in the UK post-Brexit took a video of your face as colours change on the phone screen in front of you. Most biometrics are not as accurate for all people, and some of the population can't be identified as reliably as the rest (or even at all). The elderly, and manual workers, often have damaged or abraded fingerprints; there's a tradition of hardcore criminals doing this deliberately. People with dark eyes, and large pupils, give poorer iris codes. Disabled people with no fingers, or no eyes, risk exclusion. (That's one reason Aadhaar uses both irises and fingerprints.) Illiterates who make an ‘’ are more at risk from signature forgery. Biometric engineers sometimes refer to such subjects dismissively as ‘goats’, but this is foolish and discriminatory. A biometric system that is (or is seen to be) socially regressive – that puts the</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 17: Biometrics &gt; Location 16102</div><div class='noteText'>disabled, the poor, the old and ethnic minorities at greater risk of impersonation – should meet with principled resistance. It might be defeated by legal challenges [1555]. It may also be defeated by villains who pretend to be disabled. And sometimes the lack of heed for minority population groups is so offensive as to be unlawful. For example, in 2019 the UK Home Office deployed a passport app despite knowing that it didn't work properly for black people [1954]. A point that follows from this is that systems may be vulnerable to collusion. Alice opens a bank account and her accomplice Betty withdraws money from it; Alice then complains of theft and produces a watertight alibi. Quite apart from simply letting Betty take a rubber impression of her fingertip, Alice might voluntarily decrease handwriting ability; by giving several slightly different childish sample signatures, she can force the machine to accept a lower threshold than usual. She can spend a couple of weeks building a wall in her garden, and wear her fingerprints flat, so as to degrade registration in a fingerprint system. She might register for a voice recognition system when drunk. The next issue is compulsion. If you get arrested in China, and since August 2020 in Hong Kong, the police will hold your finger to your phone to unlock it. If it uses face recognition, they'll pin your head and point your phone at you; if you want to resist, you have to close your eyes and scrunch up your face [1350]. The statistics are often not understood by system designers, and the birthday theorem is a big soft spot. With 10,000 biometrics in a database, for example, there are about 50,000,000 pairs. So even with a false-accept rate of only one in a million, the likelihood of there being at least one false match will rise above one-half as soon as there are somewhat over a thousand people enrolled4. So identification is a lot tougher than verification. The practical consequence is that a system designed for authentication may fail when you try to rely on it for evidence. Another aspect of statistics comes into play when designers assume that by combining biometrics they can get a lower error rate. But a combination will often improve either the false-accept rate or the false-reject rate, while making the other worse. If you install two different burglar alarms at your home, then the probability that they will be simultaneously defeated goes down while the number of false alarms goes up. The statistics are often somewhat uneven, so that as well as so-called ‘goats’, whose biometrics typically fall outside the normal parameter range, there may be ‘lambs’ who are particularly easy to</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16221</div><div class='noteText'>Tamper-resistant devices are everywhere now. Examples we've discussed so far include: the EMV chips used in bank cards and the SIMs used in mobile phones for authentication; the contactless cards used as transport tickets and the smartcards used in pay-TV decoders for service control; chips used for accessory control in printer toner cartridges and game-console accessories; the TPM chips in phones, laptops and servers to provide a root of trust to support secure boot and hard-disk encryption; hardware security modules used to encrypt bank PINs, not just in bank server farms but in ATMs and some point-of-sale terminals; the NFC chips used in Android phones to store contactless payment credentials, and the enclave chips in iPhones that store your fingerprint and crypto keys; cryptographic modules buried in vending machines that sell everything from railway tickets through postage stamps to the magic numbers that activate your electricity meter; various chips used for manufacturing control by firms who want to have their products made by low-cost overseas manufacturers but don't want to see extra products made without their consent on a ‘third shift’ and sold on the grey market.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16298</div><div class='noteText'>This led to the development of standalone cryptographic modules or hardware security modules (HSMs), as fintech people call them. These are microcomputers encased in robust metal enclosures, with encryption hardware and special key memory, static RAM that is zeroized when the enclosure is opened. Initially, this just involved wiring the power supply to the key memory through a number of lid switches. So whenever the maintenance crew came to replace batteries, they'd open the lid and destroy the keys. Once they'd finished, the HSM custodians would reload the key material. In this way, the HSM's owner could hope that its keys were under the unique control of its own trustworthy staff.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16304</div><div class='noteText'>How to hack a cryptoprocessor (1) The obvious attack is just to steal the keys. In early HSMs, the master keys were kept in PROMs that were loaded into a special socket in the device to be read during initialization, or as strings of numbers that were typed in at a console. The PROMs could be pocketed, taken home and read out. Cleartext paper keys were even easier: just scribble down a copy. The fix was shared control – to have two or three master key components, and make the actual master key by combining them. The PROMs (or paper keys) would be kept in different safes under the control of different departments. This taught us that shared control is a serious security usability hazard. The manual may tell the custodians to erase the live keys, let the engineer fix the device, and then re-load the keys afterwards. But many senior men used to think that touching keyboards was women's work, and even today they think that technical work is beneath them. And who reads the manual anyway? So managers often give both keys to the engineer to save the bother. In one case, a dishonest engineer got them to enter the keys using a laptop that acted as a terminal but had logging switched on [55]. I've even come across cases of paper master keys for an automatic teller machine being kept in the correspondence file in a bank branch, where any of the staff could look them up. How to hack a cryptoprocessor (2) Early devices were vulnerable to attackers cutting through the casing. Second-generation devices made physical attacks harder by adding photocells and tilt switches. But the difficult opponent is the maintenance engineer – who could disable the sensors on one visit and extract the keys on the next. By about 2000, the better products separated all the components that can be serviced (such as batteries) from the core of the device (such as the tamper sensors, cryptoprocessor, key memory and alarm circuits). The core was then potted into a solid block of a hard, opaque substance such as epoxy. The idea was that any physical attack would involve cutting or drilling, which could be detected by the guard who accompanies the engineer into the bank computer room1. At least it should leave evidence of tampering after the fact. This is the level of protection needed for medium-level evaluations under the FIPS standard. How to hack a cryptoprocessor (3) However, if a competent attacker can get unsupervised access to the device for even a short period of time – and, to be realistic, that's what the maintenance engineer probably has, as the guard doesn't understand what's going on – then potting the device core is inadequate. For example, you might scrape away the potting with a knife and drop the probe from a logic analyzer on to one of the chips. In theory, scraping the sticky epoxy should damage the components inside; in practice, it's just a matter of</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16330</div><div class='noteText'>patience. Cryptographic algorithms such as RSA, DES and AES have the property that an attacker who can monitor any bitplane during the computation can recover the key [861]. So the high-end products acquired a tamper-sensing barrier. An early example appeared in IBM's ABYSS system in the mid-1980s, which used loops of 40-gauge nichrome wire wound loosely around the device as it was embedded in epoxy, and then connected to a sensing circuit [2002]. The theory was that techniques such as milling, etching and laser ablation would break the wire, erasing the keys. But the wire-in-epoxy technique can be vulnerable to slow erosion using sand blasting; when the sensing wires become visible at the surface of the potting, shunts can be connected round them. In 2018 Sergei Skorobogatov managed to use a combination of acid etching and masking to expose a battery-powered chip, on the Vasco Digipass 270, showing that given decent lab technique you can indeed attack live circuits protected by wires in epoxy [1785]. The next major product from IBM, the 4753, used a metal shield combined with a membrane printed with a pattern of conductive ink and surrounded by a more durable material of similar chemistry. The idea was that any attack would break the membrane with high probability. The 4758 had an improved tamper-sensing membrane in which four overlapping zig-zag conducting patterns were doped into a urethane sheet, which was potted in a chemically similar substance so that an attacker cutting into the device had difficulty even detecting the conductive path, let alone connecting to it. This potting surrounds the metal shielding, which in turn contains the cryptographic core. The design is described in more detail in [1799].</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16345</div><div class='noteText'>How to hack a cryptoprocessor (4) The next class of attack exploited memory remanence, the fact that many kinds of computer memory retain some trace of data that have been stored there. Once a certain security module had run for some years using the same master keys, their values burned in to the device's static RAM. On power-up, about 90% of the relevant memory bits would assume the values of the previously stored secret keybits, which was quite enough to recover the keys [108]. Memory remanence affects not just static and dynamic RAM, but other storage media too. The relevant engineering and physics issues are discussed in [838] and [841], and in 2005 Sergei Skorobogatov discovered how to extract data from Flash memory in microcontrollers, even after it had been ‘erased’ several times [1774]; like it or not, the wear-levelling processors in Flash chips become part of your trusted computing base. RAM contents can also be burned in by ionising radiation, so radiation sensing or hardening might make sense too.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16355</div><div class='noteText'>How to hack a cryptoprocessor (5) Computer memory can also be frozen by low temperatures. By the 1980s it was realized that below about − 20 ° C, static RAM contents can persist for several seconds after power is removed. This extends to minutes at the temperatures of liquid nitrogen. So an attacker might freeze a device, remove the power, cut through the tamper sensing barrier, extract the RAM chips containing the keys, and power them up again in a test rig. In 2008, Alex Halderman and colleagues developed this into the cold boot attack on encryption keys in PCs and phones [855]. Modern DRAM retains memory contents for several seconds after power is removed, and even longer at low temperatures; by chilling memory with a freezing spray, then rebooting the device with a lightweight operating system, keys can often be read out. Software encryption of disk contents can be defeated unless there are mechanisms to zeroise the keys on power-down. Even keeping keys in special hardware such as a TPM isn't enough if all it's doing is limiting the number of times you can guess the hard disk encryption password, but then copying the master key to main memory once you get the password right so that the CPU can do the rest of the work. You need to really understand what guarantees the crypto chip is giving you – a matter we'll discuss at greater length in the chapter on advanced cryptographic engineering. Anyway, the better cryptographic devices have temperature and radiation alarms. But modern RAM chips exhibit a wide variety of memory remanence behaviors; remanence seems to have got longer as feature sizes have shrunk, and in unpredictable ways even within standard product lines. So although your product might pass a remanence test using a given make of SRAM chip, it might fail the same test with the same make of chip purchased a year later [1772]. This shows the dangers of relying on a property of some component to whose manufacturer this property is unimportant. The main constraints on the HSM alarms are similar to those we encountered with more general alarms. There's a tradeoff between the false alarm rate and the missed alarm rate, and thus between security and robustness. Vibration, power transients and electromagnetic interference can be a problem, but</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16374</div><div class='noteText'>temperature is the worst. A device that self-destructs if frozen can't be sent reliably through normal distribution channels, as aircraft holds can get as low as − 40 ° C. (We've bought crypto modules on eBay and found them dead on arrival.) Military equipment makers have the converse problem: their kit must be rated from − 55 ° to + 155 ° C. Some military devices use protective detonation; memory chips are potted in steel cans with a thermite charge precisely calculated to destroy the chip without causing gas release from the can. Meeting simultaneous targets for tamper resistance, temperature tolerance, radiation hardening, shipping safety, weight and cost can be nontrivial.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16379</div><div class='noteText'>How to hack a cryptoprocessor (6) The next set of attacks on cryptographic hardware involves monitoring the RF and other electromagnetic signals emitted by the device, or even injecting signals into it and measuring their externally visible effects. This technique, which is variously known as ‘Tempest’, ‘power analysis,’ ‘side-channel attacks’ or ‘emission security’, is such a large subject that I devote the next chapter to it. As far as the 4758 was concerned, the strategy was to have solid aluminium shielding and to low-pass filter the power supply to block the egress of any signals at the frequencies used internally for computation. This shielding is inside the tamper-sensing membrane, to prevent an opponent cutting a slot that could function as an antenna.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16386</div><div class='noteText'>How to hack a cryptoprocessor (7) We never figured out how to attack the hardware of the 4758. The attacks we have seen on high-end systems have involved the exploitation of logical rather than physical flaws. One hardware security module, the Chrysalis-ITS Luna CA3, had its key token's software reverse engineered by Mike Bond, Daniel Cvrček and Steven Murdoch who found code that enabled an unauthenticated “Customer Verification Key” to be introduced and used to certify the export of live keys [284]. Most recently, in 2019, Gabriel Campana and Jean-Baptiste Bédrune found a buffer overflow attack on the Gemalto Safenet Protect Server PSI-E2/ PSE2 by fuzzing the HSM emulator that came with its software development kit, then checked this on a real HSM, and wrote code to upload arbitrary firmware, which is persistent and can download all the secrets</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16394</div><div class='noteText'>This did not happen to IBM's 4758, which had a formally verified operating system. But most of its users ran a banking crypto application called CCA that is described in [917]. Mike Bond and I discovered that the application programming interface (API) that CCA exposed to the host contained a number of exploitable flaws. The effect was that a programmer with access to the host could send the security module a series of commands that would cause it to leak PINs or keys. These vulnerabilities were largely the legacy of previous encryption devices with which 4758 users needed to be backward compatible, and in fact most other security modules were worse. Such attacks were hard to stop, as from time to time Visa would mandate new cryptographic operations to support new payment network features and these would introduce new systemic vulnerabilities across the whole fleet of security modules [22]. Some HSMs now have two APIs: an internal one which the vendor tries to keep clean (but which needs to have the ability to import and export keys) and an external one that implements the standards of whatever industry the HSM is being used to support. The software between the two APIs may be trusted, but can be hard to make trustworthy if the external API is insecure. In effect, it has to anticipate and block API attacks. The end result is that many banks pay top dollar for secure HSMs which they use for formal compliance, while relying on other access control mechanisms to shield these precious devices from attack. There are even specialist firms selling firewalls to shield HSMs from software-based harm. I'll discuss API attacks in detail in the chapter on advanced cryptographic engineering.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16409</div><div class='noteText'>When IBM launched the 4753 in 1991, they proposed the following classification of attackers in the associated white paper [9]: Class 1 attackers – ‘clever outsiders’ – are often very intelligent but may have insufficient knowledge of the system. They may have access to only moderately sophisticated equipment. They often try to take advantage of an existing weakness in the system, rather than try to create one. Class 2 attackers – ‘knowledgeable insiders’ – have substantial specialized technical education and experience. They have varying degrees of understanding of parts of the system but potential access to most of it. They often have highly sophisticated tools and instruments for analysis. Class 3 attackers – ‘funded organizations’ – are able to assemble teams of specialists with related and complementary skills backed by great funding resources. They are capable of in-depth analysis of the system, designing sophisticated attacks, and using the most advanced analysis tools. They may use Class 2 adversaries as part of the attack team.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16504</div><div class='noteText'>reported to have been about $ 30m [1332]. How to hack a smartcard (1) The earliest hacks targeted the protocols rather than the cards themselves. For example, some early pay-TV systems gave each customer a card with access to all channels, and then sent messages over the air to cancel those channels to which the customer hadn't subscribed after an introductory period. This opened an attack in which a device was inserted between the smartcard and the decoder to intercept and discard any messages addressed to the card. So you could cancel your subscription without the vendor being able to cancel your service. The same kind of attack was launched on the German phone card system, with handmade chip cards sold in brothels and in hostels for asylum seekers [185,1817]. How to hack a smartcard (2) As smartcards use an external power supply, and store security state such as crypto keys and value counters in EEPROM, an attacker could freeze the EEPROM contents by removing the programming voltage, . Early smartcards received from the card reader on a dedicated contact. So by covering this contact with sticky tape, cardholders could prevent a value counter from being decremented. With some payphone chipcards, this gave infinite units. The fix was to generate internally from the supply voltage using a voltage multiplier. However, this isn't foolproof as the circuit can be destroyed by an attacker, for example with a laser shot. As well as bypassing value controls, they can also bypass a PIN retry counter and try every possible PIN, one after another. So a prudent programmer won't just ask for a customer PIN and decrement the counter if it fails. You decrement the counter, check it, get the PIN, verify it, and if it's correct then increment the counter again3. How to hack a smartcard (3) Another early attack was to read the voltages on the chip surface using a scanning electron microscope (SEM). The low-cost SEMs found in universities back then couldn't do voltage contrast microscopy at more than a few tens of kilohertz, so attackers would slow down the clock. In one card, attackers found they read out RAM contents with a suitable transaction after reset, as working memory wasn't zeroized. Modern smartcard processors have a watchdog timer or other circuit to detect low clock frequency and reset the card, or else use dynamic logic. And the attacker could sometimes single-step the program by repeatedly resetting the card and clocking it times, then + 1 times, and so on. But as with burglar alarms, there's a tradeoff between false alarms and missed alarms. Cheap card readers can have wild fluctuations in clock frequency when a card is powered up, causing many false alarms. Eventually, cards acquired an internal clock.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16533</div><div class='noteText'>How to hack a smartcard (4) Once pay-TV operators had blocked the easy attacks, pirates turned to physical probing. Early smartcards had no protection against physical tampering except the microscopic scale of the circuit, a thin glass passivation layer on the surface of the chip, and potting that is typically some kind of epoxy. Techniques for depackaging chips are well known, and discussed in detail in standard works on semiconductor testing, such as [198]. In most cases, a milliliter of fuming nitric acid is more than enough to dissolve the epoxy. Probing stations consist of microscopes with micromanipulators attached for landing fine probes on the surface of the chip. They are used in the semiconductor industry for testing production-line samples, and can be bought second-hand (see Figure 18.3). They may have specialized accessories, such as a laser to shoot holes in the chip's passivation layer.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16561</div><div class='noteText'>How to hack a smartcard (5) This strategy was defeated by Oliver Kömmerling's memory linearization attack in which the analyst damages the chip's instruction decoder in such a way that instructions such as jumps and calls – which change the program address other than by incrementing it – are broken [1080]. One way to do this is to drop a grounded microprobe needle on the control line to the instruction latch, so that whatever instruction happens to be there on power-up is executed repeatedly. The memory contents can now be read off the bus. In fact, once some of the device's ROM and EEPROM is understood, the attacker can skip over unwanted instructions and cause the device to execute only instructions of their choice. So with a single probing needle, they can get the card to execute arbitrary code, and in theory could get it to output its secret key material on the serial port. This can be thought of as an early version of the return-oriented programming attack. But probing the memory contents off the bus is usually more convenient. There are often several places in the instruction decoder where a grounded needle will prevent programmed changes in the control flow. So even if it isn't fully understood, memory linearization can often be achieved by trial and error. One particularly vulnerable smartcard family was the Hitachi H8/ 300 architecture, which had a 16-bit bus with the property that if the most significant bit equals 1 then the CPU will always execute single-cycle instructions without</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16576</div><div class='noteText'>Memory linearization is an example of a fault induction attack. There are many other examples. Faults can be injected into processors in many ways, from hardware probing through power transients and laser illumination. One common target is the test circuitry. A typical chip has a self-test routine in ROM that is executed in the factory and allows all the memory contents to be read and verified. In some cases, a fuse is blown in the chip to stop an attacker using the facility. But the attacker can cause a fault in this mechanism – whether by flipping a bit in Flash memory [1780], or just finding the fuse and bridging it with two probing needles [303]. In other cases, the test routine is protected with a password, which can be found [1779]. We noted in section 5.7.1 that the RSA algorithm is fragile in the presence of failure; one laser shot is all it takes to cause a signature to be right modulo and wrong modulo , enabling the attacker to factor the key . Adi Shamir pointed out that if a CPU has an error in its multiply unit – even just a single computation whose result is returned consistently wrong in a single bit – then you can design an RSA ciphertext for decryption (or an RSA plaintext for signature) so that the computation will be done correctly mod but incorrectly mod , again enabling you to factor the key [1708]. So a careful programmer will always check the results of critical computations, and think hard about what error messages might disclose.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16591</div><div class='noteText'>How to hack a smartcard (6) The next thing the pay-TV card industry tried was to incorporate hardware cryptographic processors, in order to force attackers to reconstruct hardware circuits rather than simply clone software, and to force them to use more expensive processors in their pirate cards. In the first such implementation, the crypto processor was a separate chip packaged into the card, and it had an interesting protocol failure: it would always work out the key needed to decrypt the current video stream, and then pass it to the CPU which would decide whether or not to pass it on to the</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16596</div><div class='noteText'>outside world. Hackers just tapped the wire between the two chips. The next version had the crypto hardware built into the CPU itself. Where this consists of just a few thousand gates, an attacker can trace the circuit manually from micrographs. But with larger gate counts and deep submicron processes, a successful attack needs serious tools: you need to etch or grind away the layers of the chip, take electron micrographs, and use image processing software to reconstruct the circuit [270]. Equipment can now be rented and circuit-reconstruction software can be bought; the short resource now is skilled reverse engineers. By the late 1990s, some pirates had started to get commercial reverse-engineering labs to reconstruct chips for them. Such labs get much of their business from analyzing integrated circuits on behalf of chip makers' competitors, looking for patent infringements. They also reverse chips used for accessory control, as doing this for compatibility rather than piracy is lawful. Many labs were located in Canada, where copying pay-TV cards wasn't a crime until 2002 (though there were at least two cases where these labs were sued by pay-TV operators). Some labs are now in China, whose legal system is harder for outsiders to navigate.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16607</div><div class='noteText'>How to hack a smartcard (7) In 1995 STM pioneered a new defence, a protective shield on the chip surface. This was a serpentine sensor line, zig-zagging round ground lines in a top metal layer. Any break or short would be sensed as soon as the chip was powered up, whereupon the chip would overwrite the keys. Sensor mesh shields can really push up the cost of an attack. One bypass is to hold the sensor line to with a needle, but this can be fragile; and other vendors have multiple sensor lines with real signals on them. So if you cut them, you have to repair them, and the tool for the job is the Focused Ion Beam Workstation (FIB). This is a device similar to a scanning electron microscope but which uses a beam of ions instead of electrons. By varying the beam current, it can be used either as a microscope or as a milling machine, with a useful resolution under 10 nanometers. By introducing a gas that's broken down by the ion beam, you can lay down either conductors or insulators with a precision of a few tens of nanometers. For a detailed description of FIBs and other semiconductor test equipment that can be used in reverse engineering, see [1235]. FIBs are so useful in all sorts of applications, from semiconductor testing through metallurgy and forensics to nanotechnology, that they are widely available in physics and material-science labs, and can be rented for about a hundred dollars an hour. Given such a tool, it is straightforward to attack a shield that is not powered up. The direct approach is to drill a hole through the mesh to the metal line that carries the desired signal, fill it up with insulator, drill another hole through the center of the insulator, fill it with metal, and plate a contact on top – typically a platinum ‘X’ a few microns wide, which you then contact with a needle from your probing station (see Figure 18.5). There are many more tricks, such as using the voltage contrast and backscatter modes of your electron microscope to work out exactly where to cut, so you can disable a whole section of the mesh. John Walker has a video tutorial on how to use these tricks to defeat a shield at [1979]. Many other defenses can force the attacker to do more work. Some chips have protective coatings of silicon carbide or boron nitride, which can force the FIB operator to go slowly rather than damage the chip through a build-up of electrical charge. Chips with protective coatings are on display at the NSA Museum at Fort Meade, Maryland.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16633</div><div class='noteText'>How to hack a smartcard (8) In 1998, the smartcard industry was shaken when Paul Kocher announced a new attack known as differential power analysis (DPA). This relies on the fact that different instructions consume</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16636</div><div class='noteText'>different amounts of power, so by measuring the current drawn by a chip it was possible to extract the key. Smartcard makers had known since the 1980s that this was theoretically possible, and had even patented some crude countermeasures. But Paul came up with efficient signal processing techniques that made it easy, and which I'll describe in the following chapter. He came up with even simpler attacks based on timing; if cryptographic operations don't take the same number of clock cycles, this can leak key material too4. Power and timing attacks are examples of side-channel attacks, where the opponent can observe some extra information about the processor's state during a cryptographic computation. All the smartcards on the market in 1998 turned out to be highly vulnerable to DPA, and this held up the industry's development for a couple of years while countermeasures were developed. Attacks were traditionally classed as either invasive attacks such as mechanical probing, which involves penetrating the passivation layer, and noninvasive attacks such as power analysis, which leaves the card untouched. Noninvasive attacks can be further classified into local attacks where the opponent needs access to the device, as with power analysis; and remote attacks where she could be anywhere, such as timing attacks. But that was not the whole story. How to hack a smartcard (9) Mechanical probing techniques have been getting steadily harder because of shrinking feature sizes. The next attack technology to develop was optical probing. The first report was from Sandia National Laboratories who in 1995 described a way to read out a voltage directly using a laser [33]. Since 2001 optical probing has been developed into an effective and low-cost technology, largely by my Cambridge colleague Sergei Skorobogatov. In 2002 Sergei and I reported using a photographic flashgun, mounted on the microscope of a probing station, to induce transient faults in selected transistors of an IC [1786]. The light ionises the silicon, causing transistors to conduct. Once you understand photoconductivity and learn to focus the light on single transistors, by upgrading from a flashgun to a laser, this enables many direct attacks. For example, microcontrollers can be opened by toggling the flip-flop that latches their protection state. This gave a new way of causing not just transient fault attacks, as on fragile cryptosystems such as RSA, but faults that are precisely directed and controlled in both space and time. Later in 2002, Sergei reported using a laser mounted on the same cheap microscope to read out a microcontroller's memory directly. The basic idea is simple: if you shine a laser on a transistor, that will induce a photocurrent and increase the device's power consumption – unless it was conducting already. So by scanning the laser across the device, you map which transistors are off and which are on. We developed this into a reasonably dependable way of reading out flip-flops and RAM memory [1651]. We named our attack semi-invasive analysis as it lies between the existing categories of invasive and non-invasive. It's not invasive, as we don't break the passivation; but we do remove the epoxy, so it doesn't count as non-invasive either. Optical probing from the front side of the chip remained the state of the art for about five years. By the time of this book's second edition (2007), smartcard vendors were using 0.18 and 0.13   micron processes, typically with seven metal layers. Direct optical probe attacks from the chip surface had become difficult, not so much because of the feature size but because the metal layers get in the way. In addition, the sheer size and complexity of the chips was making it difficult to know where to aim. The difficulty was increased by glue logic – essentially randomised place-and-route. Older chips have clearly distinguishable blocks, and quite a lot can be learned about their structure and organisation just by looking. Bus lines could be picked out and targeted for attack. However, the SX28 in Figure 18.6 just looks like a random sea of gates. The only easily distinguishable features are the EEPROM (at top left) and the RAM (at top right). It takes some work to find the CPU, the instruction decoder and the bus.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16726</div><div class='noteText'>Hardware random number generators (RNGs) are used to produce protocol nonces and session keys. Weak generators have led to many catastrophic security failures, of which a number pop up in this book. Poor nonces lead to replay attacks, while weak session keys can compromise long-term signing keys in cryptographic algorithms such as ECDSA. During the 1990s, the fashion was for algorithmic random number generation; this is properly known as a pseudorandom number generator (PRNG). A crypto chip might have had a special key-generation key that was used in counter encryption mode; operating systems often had something similar. However, if the counter is reset, then the output is repeated; there have been several variants on this theme. I also mentioned the NIST Dual-EC-DRBG, which was built into Windows and seemed to have contained an NSA trapdoor [1737]; Ed Snowden later confirmed that the NSA paid RSA $ 10m to use this standard in tools that many tech companies licensed [1292].</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16734</div><div class='noteText'>Hardware random number generators typically quantise jitter or use some source of metastability such as a cross-coupled inverter pair. Such generators are notoriously difficult to test; faults can be induced by external noise such as temperature, supply voltage and radiation. Standards such as NIST SP800-A/ B/ C call for RNG output to be run through test circuits. Crypto products often mix together the randomness from a number of sources both environmental and internal [839], and this is a requirement for the highest levels of certification. The way these sources are combined is often the critical thing and one should beware of designs that try to be too clever [1035]. One must also beware that hardware RNGs are usually proprietary, obscure designs, sometimes specific to a single fab, so it's hard to check that the design is sound, let alone that it doesn't contain a subtle backdoor. An example of conservative design may be that used in Intel chips since 2012, which combines both a hardware RNG and a software PRNG that follows it [857]. The manufacture of crypto chips typically involves a personalisation stage where serial numbers and crypto keys are loaded into Flash or EEPROM. This is another attack point: Ed Snowden reported that GCHQ had hacked the mechanisms used by Gemalto to personalise cards, and got copies of the keys in millions of SIMs. So one might ask whether chips could be manufactured with an intrinsic key that would never leave the device. Each chip would create a private key and export the public key, which the vendor would certify during personalisation. But this takes time, and also seems to need an RNG on the chip.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16748</div><div class='noteText'>A physical unclonable function (PUF) is a means of identifying a device from variations that occur naturally during manufacture.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16757</div><div class='noteText'>Through the 2010s we've started to see PUFs appearing in significant numbers of low-cost chips as well as in higher-value products such as FPGAs. The typical ‘weak PUF’ generates a consistent random number on power-up from process variability; an SRAM PUF reads the initial state of some SRAM cells and is used, with error correction, as a stable random ID or as an AES key to encrypt memory or to drive a PRNG. If your opponent is capable of reversing your circuit and scanning your Flash memory, a PUF may at least force them to go to the trouble of probing the key off the bus, or inducing faults one bus line at a time to read it out using differential fault analysis.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16762</div><div class='noteText'>PUF marketing often claims much more, and one claim (as well as a research goal) is a ‘strong PUF’ which would act as a hardware challenge-response mechanism. Given an input, it would return an output that would be sufficiently different for each chip (and each input) to be usable as a cryptographic primitive in itself. For example, one might send a thousand challenges to the chip at personalisation and store the responses for later key updating. Note that this would not of itself have stopped the NSA attack on Gemalto, as they hacked the personalisation files and if PUFs had been used they'd have got the challenge-response pair files too.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16767</div><div class='noteText'>The state of the art in 2020 appears to be XOR arbiter PUFs, which consist of a chain of multiplexers followed by an arbiter. The challenge to the PUF is input to the address lines of the multiplexers that select a route for signals to race through them to get to the arbiter. To make it harder for an attacker to work out the relative delay on each circuit path, the outputs of a number of arbiters are XORed together. However,</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16781</div><div class='noteText'>There's a growing number of larger chips with embedded security functions, typically aimed at manufacturing control or accessory control. The granddaddy of these products may be the Clipper chip, which the Clinton administration proposed in 1993 as a replacement for DES. Also known as the Escrowed Encryption Standard (EES), this was a tamper-resistant chip containing the Skipjack block cipher and a protocol designed to allow the FBI to decrypt any traffic encrypted using it. When you gave Clipper some plaintext and a key to encrypt it, the chip returned not just the ciphertext but also a Law Enforcement Access Field (LEAF), which contained the user-supplied key encrypted under an FBI key embedded in the device. To prevent people cheating by sending the wrong LEAF with a message, the LEAF had a MAC computed with a ‘family key’ shared by all Clipper chips – which had to be tamper-resistant to keep both the Skipjack block cipher and the LEAF family key secret. As often happens, it wasn't the tamper-resistance that failed, but the protocol. Almost as soon as Clipper hit the market, Matt Blaze found a vulnerability: as the MAC used to bind the LEAF to the message was only 16 bits long, it was possible to feed message keys into the device until you got one with a given LEAF, so a message could be sent with a LEAF that would reveal nothing to the government [259]. Clipper was replaced with the Capstone chip, the crypto wars continued by other means, and the Skipjack block cipher was placed in the public domain [1402].</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16795</div><div class='noteText'>was claimed that the Clipper chip would be unclassified and exportable, I was never able to get hold of a sample despite repeated attempts. It used Vialink read only memory (VROM) in which bits are set by blowing antifuses between the metal 1 and metal 2 layers on the chip. A high-voltage programming pulse is used to melt a conducting path through the polysilicon between two metal layers. This technology was also used in the QuickLogic FPGA, which was advertised as a way for firms to conceal proprietary algorithms, and claimed to be ‘virtually impossible to reverse engineer’; further details and micrographs appeared in its data book [802]. A recent variant is the spot breakdown PUF where a high enough voltage is applied to a bank of transistors for just long enough that about half of them suffer breakdown of the gate oxide, creating random failures that can be read as ones and zeros</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16804</div><div class='noteText'>three approaches to reverse engineering an antifuse device. The first thing to look at is the programming circuitry. All such chips have a test circuit used to read back and verify the bitstream during programming, and many disabled this by melting a single fuse afterwards. If you can get sample devices and a programmer, you can maybe find this fuse using differential optical probing [1776]. You then use a FIB to repair it, or bridge it with two probe needles, and read out the bitstream. This attack technique works not just for antifuse FPGAs but also for the Flash and EEPROM varieties. Where you need to read out many fuses, as where they're used to store an AES key, the brute-force approach is to strip the chip down one layer at a time and read the fuses directly; they turn out to be visible under a suitable chemical stain. As this attack is destructive it is typically of limited interest against keys that are different in each device (as in the iPhone, or a spot breakdown PUF). Where the device implements a cryptographic algorithm, a side-channel attack may be the fastest way in. Most devices manufactured before about 2000 are rather vulnerable to power analysis, and while smartcard chipmakers have incorporated defences, the makers of larger chips may have preferred to avoid paying royalties to Cryptography Research, which patented many of the best ones. You can always try optical fault induction to read the key one bit at a time, and since the late 2000s we also know how to work with optical emissions, which I'll discuss later.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16843</div><div class='noteText'>Memory readout can be the most dependable attack path. As an example, Sergei Skorobogatov used the new PVC Flash / EEPROM readout technique to reverse the OmniPod insulin pump. Diabetics who know how to program prefer to control their own insulin pumps but vendors try to stop them, for both market control and liability reasons. The OmniPod's system-on-chip therefore runs an authentication protocol with the device's authorised controller, and the Nightscout Foundation, an NGO that supports diabetics, wanted to extract the keys so patients could optimise the control for their own health needs rather than following the treatment protocols devised by Omnipod.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16848</div><div class='noteText'>A second attack path is to look to see whether the device computes with encrypted data, and if so look for a protocol failure or side channel that gives a way in. An early example was the cipher instruction search attack invented by Markus Kuhn on the DS5002 processor [1104]. This device pioneered bus encryption with hardware that encrypts memory addresses and contents on the fly as data are loaded and stored, so it was not limited to the small amount of RAM that could be fitted into a low-cost tamper-sensing package at the time (1995). Markus noticed that some of the processor's instructions have a visible external effect; one instruction in particular caused the next byte in memory to be output to the device's parallel port. So if you intercept the bus between the processor and memory using a test clip, you can feed in all possible 8-bit instruction bytes at some point in the instruction stream until you see a one-byte output. After using this technique to tabulate the encryption function for a few bytes, you can encipher and execute a short program to dump the entire memory contents. Similar tricks are still used today, and variants on the attack still work. In 2017 Sergei Skorobogatov demonstrated an active attack on a system-on-chip used in the car industry, which used memory encryption to make bus probing harder. By selectively injecting wrong opcodes into the bus, he was able to reverse the encryption function</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 18: Tamper Resistance &gt; Location 16872</div><div class='noteText'>The third type of attack I'll mention is optical emission analysis, which is strictly speaking a side channel but which I'll introduce here as it's becoming one of the main ways of attacking high-grade crypto chips. Photons are emitted when semiconductor junctions switch, and photon emission microscopy is an established failure analysis technique, with silicon emitting mostly in the near infrared near the drain area of n-MOS transistors. This was first used to attack a crypto implementation in 2008 by Julie Ferrigno and Martin Hlavac, who used an expensive single-photon counting photomultiplier to read out AES keys from an outdated 0.8 microcontroller, but worried that their technique would not work for technologies smaller than 0.12 [681]. By the following year, Sergei Skorobogatov found that a photomultiplier sold to hobby astronomers was near ideal and discovered a voltage boost trick: increasing the chip supply voltage from 1.5V to 2V increases the photon output sixfold. He found he was almost able to read out the AES keys from the internal crypto engine of a modern chip, the Actel ProASIC3 FGPA. Then, once the AES algorithm timing had been established, and he knew each round key took 1.6s, he further increased the voltage to 2.5V for the 0.2s of an individual bus write, giving a further fourfold increase in the photon output plus temporal resolution, which enabled him to read each word of round key clearly off the bus. This was all rather embarrassing as I'd consulted on the design to Actel back in 2001. The ProASIC3 was fabricated in a 0.13 technology with 7 metal layers and flash memory, and we had built in all sorts of countermeasures to block the attacks we knew about at the time; reading it out invasively would have been tedious. That was a sharp reminder that it's hard to block the attacks that haven't been invented yet, and that attacks can improve very quickly once experts start to hone them. Optical emission analysis is now used in combination attacks: if you want to attack a chip that's too big to reverse engineer, you observe the emissions as it does the cryptography and this tells you where to aim your laser as you try a fault attack or optically-enhanced power analysis. It can also suggest where you might lay down a few probe points with your FIB.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17140</div><div class='noteText'>A side channel is where information leaks accidentally via some medium that was not designed or intended for communication; a covert channel is where the leak is deliberate.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17165</div><div class='noteText'>Emission security, or Emsec, is about preventing attacks using compromising emanations, namely conducted or radiated electromagnetic signals. It's mostly military organizations that worry about Tempest, where the stray RF emitted by computers and other electronic equipment is picked up by an opponent and used to reconstruct the data being processed. It has become an issue for voting machines too, after a Dutch group found they could tell at a distance which party a voter had selected on a voting machine, and attacks have also been demonstrated on automatic teller machines (though these don't really scale).</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17170</div><div class='noteText'>Both active and passive emission security measures are closely related to electromagnetic compatibility (EMC) and radio frequency interference (RFI),</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17172</div><div class='noteText'>can disrupt systems accidentally, as well as electromagnetic pulse (EMP) weapons, which disrupt them deliberately.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17233</div><div class='noteText'>An expert in technical surveillance countermeasures (TSCM) will have a whole bag of tools to provide protection against such attacks. The better surveillance receivers sweep the radio spectrum from about 10   KHz to 3   GHz every few tens of seconds, and look for signals that can't be explained as broadcast, police, air traffic control and so on. Direct-sequence spread spectrum can be spotted from its power spectrum, and frequency hoppers will typically be observed at different frequencies on successive sweeps. Burst transmission does better. But the effectiveness of surveillance receivers is limited by the bugs that use the same frequencies and protocols as legitimate mobile phones. Many organizations tried to forbid the use of mobiles, but most have given up; even the Royal Navy eventually had to allow sailors to keep their phones on board ship as too many of them left. The nonlinear junction detector can find hidden devices at close range. It broadcasts a weak radio signal and listens for odd harmonics, generated when the transistors, diodes and other nonlinear junctions in the equipment rectify the signal. However, if the bug has been planted in or near legitimate equipment, then the nonlinear junction detector is not much help. There are also expensive bugs designed not to re-radiate at all. Breaking the line of sight, such as by planting trees around your laboratory, can be effective against laser microphones but is often impractical. It's possible to detect hidden wireless cameras that just use the normal building wifi by their traffic patterns, and researchers have developed apps for this purpose [417]. Some facilities have shielded rooms, so that even if bugs are introduced their signals can't be heard outside [133]. In NATO countries, Top Secret material is supposed to be kept in a secure compartmented information facility (SCIF) that has both physical security and acoustic shielding, and is swept regularly for bugs; a SCIF may have electromagnetic shielding too if a threat assessment suggests that capable motivated opponents might get close enough. Shielded rooms are required in the UK for researchers to access sensitive personal data held by government, such as tax records. There are vendors who sell prefabricated rooms with acoustic and electromagnetic shielding. But this is harder than it looks. A new US embassy building in Moscow had to be abandoned after large numbers of microphones were found in the structure, and Britain's counterintelligence service decided to tear down and rebuild a large part of a new headquarters building, at a cost of about $ 50m, after an employee of one of the building contractors was found to have past associations with the Provisional IRA.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17269</div><div class='noteText'>Broadly speaking, there are two categories of electromagnetic attack. The signal can either be conducted over some kind of circuit (such as a power line or phone line), or it may be radiated as radio frequency energy. These are referred to by the military as ‘Hijack’ and ‘Tempest’ respectively. They are not mutually exclusive; RF threats often have a conducted component. For example, radio signals emitted by a computer can be picked up by the power main and conducted into nearby buildings.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17270</div><div class='noteText'>there are two categories of electromagnetic attack. The signal can either be conducted over some kind of circuit (such as a power line or phone line), or it may be radiated as radio frequency energy. These are referred to by the military as ‘Hijack’ and ‘Tempest’ respectively.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17311</div><div class='noteText'>The zone system works as follows. Equipment certified as Zone 0 should not emit any signals that are exploitable at a distance of one meter; it should protect data from electronic eavesdropping even if the opponent is in the next room, and the wall is something flimsy like plasterboard. Zone 1 equipment should be safe from opponents at a distance of 20 meters, so the Dutch ‘Zone 1 - 12dB’ criterion means that a voting machine should not leak any data on what vote was cast to an eavesdropper 5 meters away. Zone 2 and Zone 3 mean 120 and 1200 meters respectively.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17381</div><div class='noteText'>The final class of classical Emsec attacks is the exploitation of RF emanations that are accidentally induced by nearby RF sources, called Nonstop by the US military [133]. If equipment processing sensitive data is used near a mobile phone, then the phone's transmitter may induce currents in the equipment that get modulated with sensitive data by the nonlinear junction effect and re-radiated. For this reason, it used to be forbidden to use a mobile phone within 5 meters of classified equipment. Nonstop attacks are also the main Emsec concern for ships and aircraft; here, an attacker who can get close enough to do a passive Tempest attack can probably do much more serious harm than eavesdropping, but as military ships and aircraft often carry very powerful radios and radars,</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17411</div><div class='noteText'>Butler Lampson pointed out in 1973 covert channels may allow a process at high to signal down to low [1127]. As a simple example, the high process can keep some shared resource busy at time to signal that the -th bit of a secret key is 1. If a machine is shared between high and low, and resources are not allocated in fixed slices, then the high process can signal by filling up the disk drive, or by using a lot of CPU cycles (some people call the former case a storage channel and the latter a timing channel, though in practice they can often be converted into each other). There are many others such as sequential process IDs, shared file locks and last access times on files – reimplementing all of these in a multilevel secure way is an enormous task.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17411</div><div class='noteText'>covert channels may allow a process at high to signal down to low [1127]. As a simple example, the high process can keep some shared resource busy at time to signal that the -th bit of a secret key is 1. If a machine is shared between high and low, and resources are not allocated in fixed slices, then the high process can signal by filling up the disk drive, or by using a lot of CPU cycles</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17427</div><div class='noteText'>Timing analysis In 1996, Paul Kocher showed that many implementations of public-key algorithms such as RSA and DSA leaked key information through the amount of time they took [1066]. When doing exponentiation, software typically steps through the secret exponent one bit at a time, and if the next bit is a one it does a multiply. Paul's idea was to guess the exponent one bit at a time, work through the consequences of this guess for the timing measurements, and see if it reduced their variance. This clever signal-processing technique was steadily refined. By 2003, David Brumley and Dan Boneh implemented a timing attack against Apache using OpenSSL, and showed how to extract the private key from a remote server by timing about a million decryptions</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17437</div><div class='noteText'>Symmetric-key block ciphers are vulnerable too. John Kelsey, Bruce Schneier, David Wagner and Chris Hall had pointed out in 1998 that Rijndael, the algorithm that later became AES, is vulnerable to timing attacks based on cache misses [1036]. The attacker can verify guesses about the output of the first round of the cipher by predicting whether the guessed value would cause a cache miss on S-box lookup, and verifying this against observation. A number of researchers improved this attack steadily since then, and a naïve implementation of AES can be broken by observing a few hundred encryptions [233, 1485, 1491]. Many crypto libraries and toolkits are vulnerable; you need to work out whether they are an issue for your application and if so what you're going to do. And it's not just the algorithms that leak; protocol and implementation features such as padding and error handling leak secrets too.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17450</div><div class='noteText'>Known as power analysis or rail noise analysis, this may involve as little as inserting a resistor in the ground line and connecting a digital storage scope across it to observe the device's current draw.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17454</div><div class='noteText'>Different instructions have quite different power profiles, and, as you can see, the power consumption also depends on the data being processed. The main data-dependent contribution in many circumstances is from the bus driver transistors, which are quite large. Depending on the design, the current may vary by several hundred microamps over a period of several hundred nanoseconds for each bit of the bus whose state is changed [1300]. Thus the Hamming weight of the difference between each data byte and the preceding byte on the bus (the transition count) is visible to an attacker. In some devices, the Hamming weight of each data byte is available too [1305]. EEPROM reads and writes can give even stronger signals. If a wrong PIN guess leads to a PIN-retry counter being decremented, this may cause a sharp increase in current draw as a charge pump prepares to write memory</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17472</div><div class='noteText'>differential power analysis, involves partitioning a set of power traces into subsets, then computing the difference of the averages of these subsets. If the subsets are correlated with information of interest, the difference should be nonzero [1069].</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17475</div><div class='noteText'>As a concrete example, the attacker might collect several hundred traces of transactions with a target card, for which either the plaintext or the ciphertext is known. They then guess some of the cipher's internal state. In the case of DES, each round of the cipher has eight table look-ups in which six bits of the current input are xor'ed with six bits of key, and then used to look up a four-bit output from an S-box. So if it's the ciphertext to which the attacker has access, they will guess the six input bits to an S-box in the last round. The power traces are then sorted into two sets based on this guess and synchronized. Average traces are then computed and compared. The difference between the two average traces is called a differential trace.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17481</div><div class='noteText'>The process is repeated for each of the 64 possible six-bit inputs to the target S-box. The correct input value – which separates the power traces into two sets each with a different S-box output value – will typically give a differential trace with a noticeable peak. Wrong guesses, however, give randomly-sorted traces, so the differential trace looks like random noise. In this way, the six keybits that go to the S-box in question can be found, followed by the others used in the last round of the cipher. In the case of DES, this gives 48 of the 56 keybits, so the remainder can be found trivially.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17500</div><div class='noteText'>There are many variants on the theme. Attacks based on cache misses can measure power as well as the time taken to encrypt, as a miss activates a lot of circuitry to read nonvolatile memory; you can't stop cache attacks on AES just by using a timer to ensure that each encryption takes the same number of clock cycles. Another variant is to use different sensors: David Samyde and Jean-Jacques Quisquater created electromagnetic analysis, in which they move a tiny pickup coil over the surface of the chip to pick up local signals rather than relying simply on the whole device's current draw</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17508</div><div class='noteText'>2016 when Eyal Ronen, Colin O'Flynn, Adi Shamir and Achi-Or Weingarten demonstrated a worm that could take over Philips Hue lamps, after they developed an improved power-analysis attack to retrieve the AES key that these lamps used to authenticate firmware updates [1617]. Philips had made several other mistakes: relying on a single AES key, present in millions of low-cost devices, to protect updates, using the same key for CBC and MAC, and having two bugs in the light link protocol they used. As updates could propagate by ZigBee, malware could spread in a chain reaction from one lamp to the next; the authors showed that in a city such as Paris, there were enough lamps for such a chain reaction to be self-sustaining, like nuclear fission.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17514</div><div class='noteText'>2019 is probably the template attack where the attacker studies a device's current draw closely for the instructions of interest and builds a multivariate Gaussian distribution giving the probability distribution for an observed trace given the instruction, the operands, the results and the state.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17552</div><div class='noteText'>Yoongu Kim and colleagues at CMU found that DRAM manufactured in 2012 and 2013 was vulnerable to disturbance errors; repeatedly accessing a row in a modern DRAM chip causes bit flips in physically-adjacent rows at consistently predictable bit locations, an attack now known as Rowhammer [1050]. The following year, Mark Seaborn and Thomas Dullien found how this hardware fault could be exploited by application code to gain kernel privileges [1697]. By the year after that, Kaveh Razavi and colleagues had shown how to use the technique to replace a strong public key with a weak one – with the effect that one virtual machine could attack a co-hosted target machine by subverting its OpenSSH public-key authentication, and also compromise the software update mechanism by forging GPG signatures from trusted keys [1590]. The vulnerable type of DRAM is still in such wide use and the attacks can target so many different software mechanisms, that they may be around for some time. The first generation of hardware</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17560</div><div class='noteText'>mitigation from vendors includes target row refresh (TRR) where the DRAM chip controller refreshes rows to block the most common hammering patterns; Pietro Frigo and colleagues built a fuzzer to analyse 42 chips with TRR defences, and found other patterns that gave attacks on 13 of them [725]. And in 2020, Andrew Kwong and colleagues found that the mechanism could be used to read as well as write; an attacker can exploit the dependence between Rowhammer-induced bit flips and the bits in adjacent rows to deduce those bits – and what's more, this works even when ECC memory detects and corrects each bit flip [1116].</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17567</div><div class='noteText'>In 2017 Adrian Tang, Simha Sethumadhavan, and Sal Stolfo discovered the CLKscrew attack, where they overclocked the Arm processor on a Nexus 6 to defeat TrustZone, extracting crypto keys and escalating privilege [1861]. In 2019, Kit Murdock and colleagues discovered Plundervolt: here an undocumented voltage scaling interface in Intel Core processors is exploited to cause an undervoltage that induces faults in multiply and AES-NI operations that allow RSA and AES keys to be extracted using fault analysis, as well as mistakes in pointer arithmetic that leak arbitrary memory contents from SGX exclaves</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17572</div><div class='noteText'>Although Arm and Intel released microcode patches for CLKscrew and Plundervolt, we may expect other CPU attacks of the same genre. Rowhammer / RAMBleed attacks remain an issue. In the long term, hardware security will require more defensive design. This will not be trivial: just increasing the DRAM refresh rate increases device power consumption, as would less aggressive frequency scaling. Two of the scientists who discovered Rowhammer, Onur Mutlu and Jeremie Kim, suggest that when the memory controller closes a row, then it refreshes the adjacent rows with a probability tuned to the dependability of the chip</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17584</div><div class='noteText'>Colin Percival found that AES cache misses could be used by an attacker to observe an encryption operation in another hyperthread on the same Intel CPU; by pulling data into the L1 cache, then measuring a moment later how long it takes to access the same data, you can see whether your data were evicted by the other hyperthread [1510]. Two years later, Onur Acıiçmez, Çetin Kaya Koç and Jean-Pierre Seifert invented branch prediction analysis (BPA). Modern high-performance CPUs have a superscalar architecture in which the CPU no longer fetches and executes one instruction at a time, but has a pipeline that fetches as many as a dozen instructions ahead, and tries</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17589</div><div class='noteText'>to predict which branch the code will take. BPA enabled a spy thread to extract a secret key from a parallel crypto thread by observing the CPU's branch-prediction state; a misprediction imposed a penalty of 20 cycles at the time; in the best circumstances, an RSA private key could be extracted from observing a single signature [13]. Others explored other cache behaviour; in 2015, Fangfei Liu, Yuval Yarom and colleagues showed that the L3 cache gave practical prime and probe cross-core attacks that enabled the recovery of GPG private keys [1177]. By 2017, the Cachezoom attack allowed an attacker to extract keys from SGX enclaves [1330]. The most recent such attack is the Membuster attack by Dayeol Lee and colleagues, which uses OS privilege to induce cache misses that leak data [1136]. (Intel's response has been simply to declare such attacks to be out of scope.) This was a field in which, over more than a decade of work, many ideas came together; the CPU vendors should have been paying more attention.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17599</div><div class='noteText'>The most impactful attacks were Meltdown and Spectre, disclosed in early 2018. They both exploit speculative memory reads, and build on the previous work on prime-and-probe, branch prediction and cache side-channels. They are so serious that both Intel and Arm announced that they will redesign their CPUs to block them; but that will take years, and in the meantime software mitigations (where available) may cause a 15% performance hit with some workloads, and occasional reboots.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17599</div><div class='noteText'>Meltdown and Spectre, disclosed in early 2018. They both exploit speculative memory reads, and build on the previous work on prime-and-probe, branch prediction and cache side-channels. They are so serious that both Intel and Arm announced that they will redesign their CPUs to block them;</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17603</div><div class='noteText'>Meltdown creates a race condition between memory access and privilege checking, and reads out forbidden memory via a cache side channel.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17606</div><div class='noteText'>Spectre was disclosed at the same time, having also been discovered by many of the same teams. It's actually a (growing) family of vulnerabilities exploiting the branch prediction logic that is a special case of speculative execution. This logic tries to guess which code path will be taken after a conditional jump, and rogue software can train it to mispredict. The CPU will then fetch instructions that will never be executed, and if some of these perform forbidden operations – such as when a user program reads protected kernel memory – then the protected pages may be fetched from cache. Even if they are never read – so the access-control check is never done – this gives a reliable timing side-channel that enables an attacker to observe crypto key material [1071]. In short, even if a CPU's execution is formally correct, all sorts of lower-level optimisations can make the timing depend on secret data, and a whole series of Spectre variants have come along to exploit this. While Meltdown reads a target process's data directly, Spectre tricks the target process into revealing its data via side-channels.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17615</div><div class='noteText'>The Spectre family of attacks keeps on growing; shortly after Spectre was announced, researchers discovered a variant called Foreshadow that cracks many of the features on Intel processors that Spectre didn't, including SGX and system management mode [340]. The 2019 security conferences brought a whole series of other attacks that exploit subtle microarchitectural features: Zombieload, Fallout, Smotherspectre and RAMBleed to name but four, while 2020 brought Load Value Injection, which combines ideas from Meltdown and Spectre [341], and CrossTalk, which enables one core in a CPU to attack another [1573]. Pretty well all CPUs now use branch prediction – except the tiniest – and have become so complex that there are lots of side channels. Finding them at design time isn't easy, as the tools the chipmakers developed for verifying their designs merely check that the logic gives the right answer – not how long it takes. The reason they're now being found is that the formerly sleepy backwater of microarchitectural covert channels suddenly became the hottest topic in security research, and hundreds of bright research students are suddenly looking hard. Fixing everything they find will take years, and given the nature of the technology I doubt that everything will ever be fixed. Arm, for example, has introduced new barrier instructions CSDB, SSBB and PSSBB. After CSDB appears in code, for example, no instruction may be speculatively executed using predicted data or state [132]. There's also a new data field CVS2 from v8.5A onwards to indicate the presence of mitigations against adversarial prediction training. It will take perhaps four years to get this all into silicon, and several more for the necessary support to appear in software toolchains – and longer still for programmers to learn to use it all. Many programmers won't bother, and many managers' reaction to such wicked and complex problems will be denial.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17669</div><div class='noteText'>Ilia Shumailov had figured out how one app on a mobile phone could recover passwords and PINs typed into another app by listening to the taps on the screen, using the two microphones in the device [1734]. Such time-difference-of-arrival (TDOA) processing had previously been the domain of sophisticated electronic-warfare kit; here was an application in your pocket, and that would enable a rogue app to steal your online banking password, even despite the protection available if the password entry mechanism is implemented in the Trusted Execution Environment, so malware cannot tap it directly.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17676</div><div class='noteText'>shoulder surfing, where someone watches your PIN over your shoulder at an ATM and then picks your pocket;</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17697</div><div class='noteText'>Thermal covert channels arrived in 2006, when Steven Murdoch discovered that a typical computer's clock skew, which can be measured remotely, showed diurnal variation, and realised this was a function of ambient temperature. His experiments showed that unless a machine's owner takes countermeasures, anyone who can extract accurate timestamps from it can measure its CPU load;</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 19: Side Channels &gt; Location 17705</div><div class='noteText'>smudge attack: these residues are a very effective way of breaking the pattern lock commonly used on Android devices [146]. (Smudges also help guess the PINs used on all sorts of touchscreen devices – even your Tesla.)</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 20: Advanced Cryptographic Engineering &gt; Location 17836</div><div class='noteText'>full-disk encryption (FDE) is simple. You encrypt data as it's written to disk, and do decryption as it's read again. The key depends on an initial authentication step such as a password, which is forgotten when the machine sleeps or is switched off. So if a doctor leaves their laptop on a train, only the hardware is lost; the medical records are not. FDE has become a regulatory requirement in many industries. In Europe, privacy regulators generally see the loss of machines with FDE as not serious enough to attract a fine or to need mandatory notification of data subjects. Many phones and laptops come with FDE; with some it's enabled by default (Android) while with others it just takes a click (Mac).</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 20: Advanced Cryptographic Engineering &gt; Location 17859</div><div class='noteText'>Yet attacks continue. In 2008, Alex Halderman and colleagues at Princeton came up with cold boot attacks, which defeated the principal FDE products then on the market and can still present a problem for many machines [855]. As I described in section 18.3, you freeze a computer's DRAM in which the transient encryption key is stored, then reboot the device with a lightweight operating system and acquire a memory image, from which the key can be read. In 2015, we found that most Androids were insecure: the factory reset function was so badly engineered by most OEMs that credentials, including FDE keys, could be recovered from second-hand devices [1761]. And most Android phones don't get patched once they're no longer on sale. And in 2019, Carlo Meijer and Bernard van Gastel found that the three third-party FDE products that held 60% of the market were insecure, that open-source software encryption would have been better, and that BitLocker turned itself off if one of these hardware products appeared to be present; thanks to their work, it no longer does so [1287]. And then there's the collateral damage. Now that lots of sensitive data are kept not on hard disks but in Amazon S3 buckets, auditors routinely demand that these buckets are encrypted; but as the failure mode of an S3 bucket isn't a burglar in Amazon's data centre but negligence over access controls, it's unclear that S3 bucket encryption achieves anything other than tick-box compliance.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 20: Advanced Cryptographic Engineering &gt; Location 17871</div><div class='noteText'>finally one has to consider abusability, of which there are at least two significant kinds. First, the wide availability of FDE code is one of the two components that led to the recent wave of ransomware attacks, where a gang penetrates your systems, installs FDE, lets it run until you've encrypted enough backups to make recovery painful, then demands a ransom for the key. (The other component is cryptocurrency, which I'll discuss later in this chapter.) Second, many people consider FDE to be magic insurance against compromise, and won't report a laptop left on a train if it had FDE enabled (or was supposed to), even if the finder might have seen the password or be able to guess it.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 20: Advanced Cryptographic Engineering &gt; Location 17890</div><div class='noteText'>The Signal protocol therefore provides the properties of forward secrecy, that a key compromise today won't expose any future traffic, and backward secrecy, which means that it won't expose previous traffic either. These are now formalised as post-compromise security</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 20: Advanced Cryptographic Engineering &gt; Location 17893</div><div class='noteText'>The protocol has three main components: the Extended Triple Diffie-Hellman (X3DH) protocol to set up keys between Alice, Bob and the server; a ratchet protocol to derive message keys once a secret key is established; and mechanisms for finding the Signal keys of other people in your address book. We can't use vanilla Diffie-Hellman to establish a fresh key between Alice and Bob, as they might not be online at the same time. So in the X3DH protocol [1229], each user publishes an identity key and a prekey to a server, together with a signature on the latter that can be verified using the former. The algorithms are elliptic-curve Diffie-Hellman and elliptic-curve DSA. When Alice wants to send a message to Bob, she fetches Bob's keys and from the server, generates an ephemeral Diffie-Hellman key , and combines them with Bob's keys in all the feasible ways: , , and . These are hashed together to give a fresh key . Alice then sends Bob an initial message containing her keys and , a note of which of Bob's prekeys she used, and a ciphertext encrypted using so that he can check he's got it too. Optionally, Bob can upload a one-time ephemeral key that Alice will combine with and hash into the mix. Given an initial Diffie-Hellman key , Alice and Bob then use the double ratchet algorithm to derive message keys for individual texts and calls. Its purpose is to recover security if one of their phones is compromised. It uses two mechanisms: a key derivation function (KDF) or one-way hash function to update stored secret keys, and further Diffie-Hellman key exchanges. Alice and Bob each maintain separate KDF chains for sending and for receiving, each with a shared-secret key and a Diffie-Hellman key. Each message carries a new Diffie Hellman key part which is combined with the key for the relevant chain, while the shared-secret key is passed through the KDF. The actual details are slightly more fiddly, because of the need to deal with out-of-order messages [1514]. The goal is that an opponent must compromise either Alice's phone or Bob's continuously in order to get access to the traffic between them. The really tricky part is the initial authentication step. If Charlie could take over the server and send Alice his own instead of Bob's, all bets are off. This is the attack being mounted on messaging apps by some intelligence agencies. Systems such as Apple's iMessage don't just send a single identity key to your counterparty but a whole keyring of device keys – one for each of your MacBooks, iPhones and other Apple devices. Ian Levy and Crispin Robinson of GCHQ propose that laws such as the UK's Investigatory Powers Bill be used to compel providers to add an extra law-enforcement key to the keyring of any user against whom they get a warrant [1155]. This has led to policy tussles in the USA, the UK and elsewhere, to which I return in section 26.2.7.4. Signal attempts to forestall such attacks by being open source, so that Alice and Bob can more easily work out whether their private conversation has been joined by Charlie as a silent conference call partner, or ‘ghost user’. Keeping such surveillance covert may be easier if the phone app software remains opaque. The upshot is that if Charlie wants to exchange Signal messages with Alice while pretending to be Bob, he has to either compromise Bob's phone or steal Bob's phone number. The options are much the same as if he wanted to steal money from Bob's bank account. They include hacking and stealing the phone; using SS7 exploits to steal Bob's SMS messages; and a SIM swap attack to take over Bob's phone number. The easiest attack for an individual to mount is probably SIM swapping, which we discussed in section 12.7.4. Signal now offers an additional PIN that you need to enter when recovering service on a phone number on which a different handset was previously active. But nation states have sophisticated hacking tools, and have SS7 access. So if the FSB's in your threat model, it's best to use a phone whose number they don't know, and don't carry it around switched on at the same time as a phone they do know is yours, or they might correlate the traces – as I described in section 2.2.1.10.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 20: Advanced Cryptographic Engineering &gt; Location 18083</div><div class='noteText'>The attack was to combine a key with itself, yielding a known key – the key of all zeros – marked as a non-exportable working key. As there was a further transaction, which would encrypt any non-exportable working key with any other, you were now home and dry. You could extract the crown jewels – the PIN derivation key – by encrypting it with your all-zero key. You can now decrypt the PIN derivation key and work out the PIN for any customer account. The HSM has been defeated. The above attack went undiscovered for years. The documentation did not spell out what the various types of key in the device were supposed to do; non-exportable working keys were just described as ‘keys supplied encrypted under master keys 14 and 15’, and the implications of a transaction to encrypt one such key under another were not immediately obvious. In fact, the HSMs had simply evolved from earlier, simpler designs as ATM networking was introduced in the 1980s and banks asked for lots more features so they could make heterogeneous networks talk to each other.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 20: Advanced Cryptographic Engineering &gt; Location 18184</div><div class='noteText'>20.6 Enclaves Enclaves are like HSMs in that they aim to provide a platform on which you can do some computation securely on a machine operated by someone you don't entirely trust. Early attempts involved mechanisms for digital rights management (DRM), which obfuscated code to make it hard to interfere with; I discuss this further in the chapter on copyright. They were followed by the ‘trusted computing’ initiative of the early 2000s, which proposed an architecture in which CPUs would execute encrypted code, with the keys stored in a separate Trusted Platform Module (TPM) chip. Arm duly produced TrustZone in 2004,</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 21: Network Attack and Defence &gt; Location 18736</div><div class='noteText'>The main technical BGP security mechanism at present is the Resource Public Key Infrastructure (RPKI), which enables registries to certify that “Autonomous system X announces IP address range Y”. This will not prevent capable attackers, as a malicious route announcement will just have the right AS at the end of the route following the attacker's in the middle; but it detects the fat-finger mistakes that cause most of the outages. Whether it will make an already fragile BGP system more robust to have lots of certificates in it remains to be seen; when RIPE's certificate expired in February 2020 there was a short outage until it was fixed.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 21: Network Attack and Defence &gt; Location 18750</div><div class='noteText'>Hijacking does occur from time to time, and at various levels. Some states intercept and redirect DNS queries as a means of censorship; some ISPs have done so, as a means of replacing ads in web pages with ads from which they get a cut; and a DNS server at an ISP may be hacked to drive clients to a wicked website. This is known as pharming, and in a variant called drive-by pharming, the crooks lure you to a web page containing JavaScript that changes your home router's DNS server from the one at your ISP to one under their control [1819]. Next time you try to go to www.citibank.com, you may be directed to a phishing site that emulates it.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 21: Network Attack and Defence &gt; Location 18758</div><div class='noteText'>In order to prevent DNS hijacking, DNSSEC adds digital signatures to DNS name records. By verifying such a signature you can check that the record came from the authoritative server and was not altered en route. Uptake is patchy: all US government domains in .gov are supposed to be signed, and most domains in Sweden are signed, as the registrar made signed domains cheaper. However, some major firms like Google don't sign their DNS records out of concern that cryptography makes systems more fragile; if anything goes wrong, you can just disappear. Other firms avoid DNSSEC because they don't want competitors to ‘walk the zone’ and enumerate all their subdomains; the NSEC3 extension enables firms to avoid this using hashes, but many firms (or their service providers) have not yet built the infrastructure. Another problem with DNSSEC is that it gets abused in denial-of-service attacks. A common technique is that Alice attacks Bob by sending Charlie a message saying, “Hey, can you tell me the very large answer to this short question? Yours, Bob!” As signed DNS records are a lot larger, a DDoS-for-hire service can use DNSSEC as an amplifier, Alice can send packets that purport to come from Bob's IP address to many DNS servers, which then bombard the target with replies. (Cheeky criminals use the FBI as Charlie, as fbi.gov has two nice big keys.)</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 21: Network Attack and Defence &gt; Location 18769</div><div class='noteText'>The controversial issue in 2020 is DNS-over-https (DoH). The main browser maintainers, Chrome and Mozilla, propose that rather than sending DNS traffic in the clear, it will go encrypted over https to a DoH resolver. This is claimed to be good for privacy, as your ISP will have less information about your browsing (but unless you use Tor, it will still have plenty). The downside is that many enterprise security products monitor DNS to detect abuse. If malware compromises a machine in your fleet, you may spot it when it tries to contact a command-and-control server, so enterprises buy threat intelligence feeds and monitor the domain names (and IP addresses) blacklisted on them. Sysadmins also like to monitor for DNS hijacking, and to block certain domains as inappropriate for work. DoH will make all this harder, and is questionable architecture as running a core network service over an application means it's ‘not the Internet any more’ [430]. On the commercial side, DoH may entrench Google's grip on the advertising market, while causing problems for content delivery networks like Akamai and Cloudflare over routing, load balancing and so on. It will also stop ISPs transcoding videos for mobile users to save bandwidth. Experts would have preferred to run DNS over TLS instead.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 21: Network Attack and Defence &gt; Location 18800</div><div class='noteText'>The more common attack now is SYN reflection. Alice sends Bob a packet that purports to come from Charlie. Bob replies to Charlie, and in practice systems send up to five ACKs in response to each SYN as a robustness measure, so there's still a useful amplification effect.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 21: Network Attack and Defence &gt; Location 18806</div><div class='noteText'>An early favourite was smurfing; this exploited the Internet control message protocol (ICMP), which enables users to send an echo packet to a remote host to check whether it's alive. If Alice sent an ICMP packet purporting to come from Bob to a broadcast address, all the machines on the subnet would send him a response.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 21: Network Attack and Defence &gt; Location 18842</div><div class='noteText'>There are two main countermeasures to bulk interception. First, most mail servers use starttls to set up encrypted communications with other mail servers as they exchange mail, especially since the Snowden revelations. Encrypted exchanges can be blocked by man-in-the-middle attacks, and these have been reported in some less-democratic countries. The current countermeasure to such attacks, MTA Strict Transport Security (MTA-STS), is supported by Microsoft, Google and Yahoo [1222]: it allows mail service providers to specify that mail should only be delivered to them via a TLS session authenticated by a proper certificate which you download from their website. This prevents downgrade or interception attacks on email to and from the big boys, and also allows opportunistic, trust-on-first-use encryption to other servers. MTA-STS has generally supplanted an earlier standard, DNS-based Authentication of Named Entities (DANE) which put a TLS certificate for starttls in the mail server's DNS record1. The second countermeasure is that some 95% of personal email accounts nowadays are at the big five webmail providers, and many corporates use them too. In this case, the confidentiality of email is assured by TLS, fortified with certificate pinning and certificate transparency which we'll discuss later. But although bulk access may be blocked, webmail is subject to warranted access, just like other services that corporates outsource.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 21: Network Attack and Defence &gt; Location 18855</div><div class='noteText'>Bulk unwanted mail, or spam, has two components. The first is entirely legal but unwanted marketing communication. As marketers can make it tiresome to opt out, users find it more convenient to press the ‘report spam’ button once an offer or supplier is no longer of interest. The second consists of floods of generally unwanted traffic sent out for the most part by botnets, and often with clear criminal intent. This is in some respects similar to a DDoS attack: just as DDoS bots may forge IP addresses, spam bots may forge the sender's email address. This is fought by the big providers with four main mechanisms. Domain Keys Identified Mail (DKIM) ties email to the sending domain by signing it using a signature key whose public verification key is kept in the sending domain's DNS record. The signed material is selected to identify the message unambiguously despite the additions to headers that occur during transit, but to stop the bad guys adding an extra “From: PayPal” header. Mail that hasn't been altered too much can be forwarded. There's a replay attack in that the spammer sends his spam through Gmail, which signs it, and then forwards it afterwards; so mail servers cache DKIM signatures and discard mail carrying a signature that's already been seen a few times. Sender Policy Framework (SPF) is similar but ties mail to the source IP address. Again, this is verifiable against a key in the domain DNS record. SPF doesn't allow mail forwarding; mailing list servers are supposed to use a related protocol called Authenticated Received Chain (ARC) to re-sign mail they forward. A domain's DNS can also contain a Domain-based Message Authentication, Reporting and Conformance (DMARC) record, which enables its owner to recommend what a recipient should do with email that appears to come from the owner's domain but which fails authentication using both DKIM and SPF. Machine-learning systems are used to filter mail against authentication results and other criteria, and take much of their ground truth from whether users report mail as spam. This is made more complicated by user preferences for marketing material, which vary by user and over time.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 21: Network Attack and Defence &gt; Location 18883</div><div class='noteText'>A worm is a malicious program that replicates itself on other systems, while one that does so by hooking itself into the code of other programs is a virus. A remote access Trojan (RAT) is software that may or may not run as root but that enables a remote party to access the device it runs</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 21: Network Attack and Defence &gt; Location 18886</div><div class='noteText'>on, while a rootkit is software installed as root on a device and that stealthily enables a third party to control it. Potentially unwanted software (PUS) may have been installed openly or by deception, but does something the user doesn't want (if they understand it at all).</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 21: Network Attack and Defence &gt; Location 18952</div><div class='noteText'>The next development was flash worms which propagate by scanning the whole Internet for machines vulnerable to some exploit or other, and taking them over; examples such as Code Red and Slammer infected all vulnerable machines within hours or even minutes, and drove research into what sort of automated defences might react in time [1824].</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 21: Network Attack and Defence &gt; Location 18983</div><div class='noteText'>Malware typically has two components – a replication mechanism or dropper, and a payload. A worm simply makes a copy of itself somewhere else when it's run, perhaps by breaking into another system by password guessing or using a remote code execution vulnerability (both of which were used by the Internet worm). Viruses spread in other software, perhaps as macros in documents, while Trojans are typically executed by the victim.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 21: Network Attack and Defence &gt; Location 18986</div><div class='noteText'>The second component of a virus is the payload. When activated, this may do one or more of a number of bad things: exfiltrate your confidential data;</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 21: Network Attack and Defence &gt; Location 18988</div><div class='noteText'>attack you directly using banking malware or spyware; encrypt your data and demand a ransom; attack others, such as when GCHQ's Operation Socialist described in section 2.2.1.9 subverted Belgacom and installed software in it to do surveillance of mobile-phone traffic passing through Belgium to other countries; perform some other nefarious task, such as using the CPU to mine cryptocurrency; install a rootkit or remote access Trojan to enable its controllers to do any of the above things, to coordinate attacks with malware on other machines, and to update itself in response to any countermeasures.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 21: Network Attack and Defence &gt; Location 19086</div><div class='noteText'>A firewall is a machine that stands between a private network and the Internet, and filters out traffic that might be harmful. It's named after the metal bulkhead that separates the passenger compartment of a car or light plane from the engine compartment, to protect the occupants from a fuel fire. Firewalls were controversial when they appeared in the mid-1990s; purists said that all the machines in a company should be secured, while firewall advocates said this was impractical. The debate has swung back and forth since.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 21: Network Attack and Defence &gt; Location 19116</div><div class='noteText'>The next step up is a circuit gateway that reassembles and examines all the packets in each TCP session. This is more expensive than simple packet filtering but can also provide the added functionality of a virtual private network (VPN) whereby corporate traffic passed over the Internet is encrypted from firewall to firewall.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 21: Network Attack and Defence &gt; Location 19124</div><div class='noteText'>The third type of firewall is the application proxy, which understands one or more services. Examples are mail filters that try to weed out spam, and web proxies that block or remove undesirable content.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 21: Network Attack and Defence &gt; Location 19362</div><div class='noteText'>HomePlug is a protocol used for communication over the mains power cables. HomePlug AV is widely used in WiFi extenders: you plug one station into your router or cable modem, and another gives a remote WiFi access point at the other end of your house.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 21: Network Attack and Defence &gt; Location 19376</div><div class='noteText'>Virtual private networks (VPNs) typically do encryption and authentication at the IP layer using a protocol suite known as IPsec. This defines a security association</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 21: Network Attack and Defence &gt; Location 19408</div><div class='noteText'>There is frequent semantic confusion between ‘public (key infrastructure)’ and ‘( public key) infrastructure’. In the first, the infrastructure can be used by whatever new applications come along; I'll call this an open PKI. In the second, it can't; I'll call this a closed PKI. If you're building a service that government agencies are likely to attack, then it may be a good idea to keep your PKI closed, with a CA that runs on your own premises – so you get to know of any warrants. I advise firms who maintain software that's installed on many millions of machines to use a private CA for their code signing keys.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 23: Electronic and Information Warfare &gt; Location 20590</div><div class='noteText'>Basics While old-fashioned computer security was about confidentiality, integrity and availability, electronic warfare has this the other way round. The priorities are: denial of service, which includes jamming, mimicry and physical attack; deception, which may be targeted at automated systems or at people; and exploitation, which includes not just eavesdropping but obtaining any operationally valuable information from the enemy's use of his electronic systems. At the level of doctrine, electromagnetic warfare is generally considered to consist of electronic attack, such as jamming enemy communications or radar, and disrupting enemy equipment using high-power microwaves; electronic protection, which is about retaining some radar and communications capability in the face of attack. It ranges from designing systems resistant to jamming, through hardening equipment to resist high-power microwave attack, to the destruction of enemy jammers using anti-radiation missiles; and electronic support, which supplies the necessary intelligence and threat recognition to allow effective attack and protection. It allows commanders to search for, identify and locate sources of intentional and unintentional electromagnetic energy.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 23: Electronic and Information Warfare &gt; Location 20684</div><div class='noteText'>the snowball search. If you suspect Alice of espionage (or drug dealing, or whatever), you note everyone she calls, and everyone who calls her. This gives you a list of dozens of suspects. You eliminate the likes of banks and doctors, who receive calls from too many people to analyze, and repeat the procedure on each remaining number.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 23: Electronic and Information Warfare &gt; Location 20811</div><div class='noteText'>In direct-sequence spread spectrum, we multiply the information-bearing sequence by a much higher rate pseudorandom sequence, usually generated by some kind of stream cipher</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 23: Electronic and Information Warfare &gt; Location 20817</div><div class='noteText'>DSSS can give substantial jamming margin (the two systems have the same theoretical performance). But it can also make the signal significantly harder to intercept. The trick is to arrange things so that at the intercept location, the signal strength is so low that it is lost in the noise floor unless the opponent knows the spreading sequence with which to recover it. Of course, it's harder to do both at the same time, since an antijam signal should be high power and an LPI/ LPPF signal low power; the usual tactic is to work in LPI mode until detected by the enemy (for example, when coming within radar range) and then boost transmitter power into antijam mode.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 23: Electronic and Information Warfare &gt; Location 20824</div><div class='noteText'>DSSS is sometimes referred to as “encrypting the RF”, and it comes in a number of variants.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 23: Electronic and Information Warfare &gt; Location 20841</div><div class='noteText'>Burst communications Burst communications, as their name suggests, involve compressing the data and transmitting it in short bursts at times unpredictable by the enemy. They are also known as time-hop. They are usually not so jam-resistant (except insofar as the higher data rate spreads the spectrum) but can be even more difficult to detect than DSSS; if the duty cycle is low, a sweep receiver can easily miss them. They are often used in radios for special forces and intelligence agents. Really high-grade room bugs often use burst.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 23: Electronic and Information Warfare &gt; Location 20846</div><div class='noteText'>An interesting variant is meteor burst transmission (also known as meteor scatter). This relies on the billions of micrometeorites that strike the Earth's atmosphere each day, each leaving a long ionization trail that persists for typically a third of a second and provides a temporary transmission path between a mother station and an area of maybe a hundred miles long and a few miles wide. The</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 23: Electronic and Information Warfare &gt; Location 20906</div><div class='noteText'>GPS spoofing takes slightly more work. An example is meaconing, where you sample the signals at location A and retransmit them at location B (this is also known as a wormhole attack). The result is that anyone near B thinks they're near A instead. This is used as a defensive mechanism in the limousines of some heads of government (a sophisticated assassin could use this to target a missile). Some countries engage in systematic GPS jamming, an example being Russia along its border with Norway. Spoofing can be largely detected using differential GPS, where you use another receiver at a known location as a reference point (the FAA's trick), and with interferometric GPS, also known as S-GPS, where you use the signals captured by successive readings by the same receiver to produce a synthetic aperture. This also increases sensitivity and deals with multipath in urban canyons, the main source of large errors in current equipment1.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 23: Electronic and Information Warfare &gt; Location 20941</div><div class='noteText'>Doppler radar measures the velocity of the target by the change in frequency in the return signal. It is very important in distinguishing moving targets from clutter, the returns reflected from the ground. Doppler radars may have velocity gates that restrict attention to targets whose radial speed with respect to the antenna is within certain limits.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 23: Electronic and Information Warfare &gt; Location 21012</div><div class='noteText'>Pulse compression was first developed in Germany in World War 2, and uses a kind of direct sequence spread spectrum pulse, filtered on return by a matched filter to compress it again. This can give processing gains of 10– 1000. Pulse compression radars are resistant to transponder jammers, but are vulnerable to repeater jammers, especially those with digital radio frequency memory. However, the use of LPI waveforms is important if you don't wish the target to detect you long before you detect it.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 23: Electronic and Information Warfare &gt; Location 21016</div><div class='noteText'>Pulsed Doppler is much the same as Doppler, and sends a series of phase stable pulses. It has come to dominate many high-end markets, and is widely</h3>
<h2 class='sectionHeading'>PART III</h2><h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 27: Secure Systems Development &gt; Location 25280</div><div class='noteText'>Critical computer systems are those in which a certain class of failure is to be avoided if at all possible.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 27: Secure Systems Development &gt; Location 25328</div><div class='noteText'>fault tree analysis where a tree is constructed whose root is the undesired behavior and whose successive nodes are its possible causes. This top-down approach is natural where you have a complex system with a small number of well-known bad outcomes that you have to avoid. It carries over in a natural way to security engineering. Figure 27.3 shows an example of a fault tree (or threat tree, as it's often called in security engineering) for fraud from automatic teller machines.</h3>
<h3 class='noteHeading'>Highlight (<span class='highlight_pink'>pink</span>) - CHAPTER 27: Secure Systems Development &gt; Location 25345</div><div class='noteText'>failure modes and effects analysis (FMEA), pioneered by NASA, which is bottom-up rather than top-down1. This involves tracing the consequences of a failure of each of the system's components all the way up to the effect on the mission.</h3>
</div> 
</body> 
</html> 
